<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>xyz :: FIXME Course Title</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="../chapter3/index.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">FIXME Course Title</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="REPLACEREPONAME" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">FIXME Course Title</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/section1.html">LLM-D Technical Reference</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section2.html">Round Robin versus LLM-Way</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section3.html">Use Case Identification</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section4.html">LLM-D Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section5.html">LLM-D Features</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section6.html">LLM-D Benefits</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">index.adoc</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section1.html">xyz</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">From vLLM to AI Factory: Scaling with LLM-D</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section1.html">LLM-D Benefits</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section2.html">Use Case Identification</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section3.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section4.html">Distributed Inference Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section5.html">Lab: Deploying Your First Distributed Inference Service</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">appendix.adoc</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">FIXME Course Title</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">FIXME Course Title</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">FIXME Course Title</a></li>
    <li><a href="index.html">index.adoc</a></li>
    <li><a href="section1.html">xyz</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">xyz</h1>
<div class="paragraph">
<p>[sidebar]Input Traffic:Request A (Small): "Translate 'hello'"Request B (Large): "Summarize this 10-page doc"Action:The load balancer sends Request A to Server 1 and Request B to Server 2.Result:
Server 1 is done in 1 second and is now IDLE.
Server 2 is 100% busy for 30 seconds, BLOCKING all new requests.This "shape-blind" routing leads to massive inefficiencies, resource bottlenecks, and idle, wasted GPU cycles.=== 2. "Stateless" Routing for a "Stateful" WorkloadLLM inference is a stateful operation. To generate a reply, the model must have the conversation history (the KV Cache) in its memory. Re-calculating this cache is the most expensive part of the process..Visual: A "Stateless" Router
[sidebar]Input Traffic:Msg 1 (Chat A): "What&#8217;s the capital of France?"Msg 2 (Chat A): "What is its population?"Action:Msg 1 hits Server 1 (which now holds the KV Cache for Chat A).The "stateless" router sends Msg 2 to Server 2.Result:
Server 2 has no context! It must re-calculate the entire cache for Chat A from scratch. This is a massive waste of GPU cycles and dramatically slows down the response (Time-To-First-Token).== The Solution: How OpenShift AI DeliversWhen you can get cheap tokens from a public API, why build your own AI factory?The answer is Performance, Security, and Cost (TCO) across any infrastructure or cloud provider.OpenShift AI&#8217;s distributed inference feature, powered by LLM-D, transforms the monolithic "black box" into an intelligent, distributed system.=== 1. Guarantees Performance &amp; SLOsBy intelligently separating large "Prefill" jobs (like Request B) from small "Decode" jobs (like Request A), nothing gets blocked. Its "Prefix Cache-Aware" routing (the opposite of stateless) sends follow-up messages to the correct server with the cache, nearly eliminating redundant calculations.=== 2. Lowers Your Total Cost of Ownership (TCO)This intelligent routing stops the waste. You stop paying for idle GPUs. By maximizing the utilization of your hardware, you get the full value from your investment, which is the key to cost-effective token generation on-premise.=== 3. Runs Securely on Your Hybrid CloudThis entire stack runs on your OpenShift cluster, on your infrastructure, whether on-prem or in the cloud. Your proprietary data never leaves your control, satisfying the strictest security and compliance needs.== Your Next Step: The "Well-Lit Paths"This lab will guide you through the most important of these "well-lit paths": Inference Scheduling.This is the core strategy that provides most of the value:Disaggregated Inference: The core concept of separating Prefill (big jobs) from Decode (small jobs).Smarter Scheduling: The "brain" (Inference Gateway) that routes traffic to the correct pod based on load and KV Cache state.Scaling MoE: An advanced path for running massive Mixture of Experts (MoE) models (not covered in this initial lab).You now understand the "Why" behind OpenShift AI&#8217;s distributed inference. It&#8217;s time to build it.Please proceed to the README.md file in this repository to begin the hands-on lab.</p>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">index.adoc</a></span>
  <span class="next"><a href="../chapter3/index.html">From vLLM to AI Factory: Scaling with LLM-D</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
