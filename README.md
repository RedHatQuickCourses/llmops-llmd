From vLLM to AI Factory: Scaling with llm-d

Increase Performance & Reduce Cost with Distributed Inference

The Value of This Course

Public APIs offer convenience, but they fail to answer the three core challenges of enterprise AI: Cost, Performance, and Security.

This course is a comprehensive, value-driven guide to solving all three. You will learn how OpenShift AI and Distributed Inference (powered by llm-d) give you what public APIs cannot:

- The Best TCO: Learn to eliminate "idle GPU" waste and maximize your hardware investment, drastically lowering your Total Cost of Ownership.

- Guaranteed Performance: Move from "best effort" to "guaranteed SLOs." Learn how to solve hotspots and ensure a fast, responsive experience for every user.

- True Hybrid Cloud Security: Master the "Sovereign AI" model. Learn to deploy world-class inference inside your own cluster, keeping your proprietary data 100% secure.

What You Will Learn

This repository contains a complete learning path, from high-level value to hands-on deployment.

- Master the "Why": Articulate the business value (TCO, ROI, Security) of on-premise, distributed inference.

- Identify Key Use Cases: Learn to identify which AI workloads are prime candidates for llm-d and how to scope their deployment.

- Explore the "Well-Lit Paths": Get a deep dive into the four pre-packaged llm-d deployment patterns that solve real-world scaling challenges.

- Understand the Architecture: Get a clear overview of the components, from the intelligent KServe Gateway and scheduler to the vLLM worker pods.

- Get Hands-On: Move from theory to practice by deploying a fully functional, auto-scaling inference service on OpenShift.

Course Structure

This course is broken down into the following key modules:

- llm-d Benefits & Value: The core business case for llm-d vs. public APIs.

- Use-Case Identification: A framework for identifying high-impact AI workloads.

- The "Well-Lit Paths": A deep dive into the four llm-d deployment patterns.

- Architecture Overview: How the components (Gateway, Scheduler, vLLM) work together.

- Hands-On Lab: Deploying llm-d on OpenShift AI. "Coming Soon"

- Hands-On Interactive Lab

The centerpiece of this course is a fully interactive, Arcade-based lab. This walkthrough guides you through the entire llm-d deployment on OpenShift / OpenShift AI.

You will see and build a live deployment that demonstrates:

- The Intelligent Inference Scheduler

- Smart Routing based on real-time pod load

- Auto-scaling LLM workers to meet demand

- Troubleshooting common resource constraints

Prerequisites

This course is designed for technical professionals, including Solutions Architects, Consultants, and Technical Sellers. It assumes a foundational understanding of OpenShift Container Platform Cluster (Kubernetes) and the core concepts of vLLM Inference.