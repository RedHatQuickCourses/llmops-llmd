<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab Demo: llm-d Deployment Guided Arcade Experience :: Maximize Your GPU ROI, Scaling LLM Inference llm-d</title>
    <link rel="prev" href="benefits.html">
    <link rel="next" href="../appendix/appendix.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="redhatai.html">AI Factory on Red Hat AI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="llmdarch.html">llm-d Architecture Deep Dive: Building the Foundation</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="benefits.html">llm-d Use Cases and Core Benefits</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="section6.html">Lab Demo: llm-d Deployment Guided Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/trbleshoot.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Maximize Your GPU ROI, Scaling LLM Inference llm-d</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></li>
    <li><a href="section6.html">Lab Demo: llm-d Deployment Guided Arcade Experience</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab Demo: llm-d Deployment Guided Arcade Experience</h1>
<div id="preamble">
<div class="sectionbody">
<div class="admonitionblock note icon-cogs">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Your Starting Point</div>
This page provides a series of interactive (Arcade) walkthroughs to help you provision and configure your OpenShift and OpenShift AI environment. You will use the Red Hat Demo Platform (RHDP) to order your cluster, configure the necessary components, and deploy the a small Granite 4 using distributed inference with llm-d on a single GPU.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_follow_these_six_steps_in_order_to_complete_the_lab"><a class="anchor" href="#_follow_these_six_steps_in_order_to_complete_the_lab"></a>Follow these six steps in order to complete the lab.</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_1_requesting_the_lab_environment"><a class="anchor" href="#_1_requesting_the_lab_environment"></a>1. Requesting the Lab Environment</h3>
<div class="paragraph">
<p>This first Arcade experience demonstrates how to request your lab environment from the Red Hat Demo Platform (RHDP) catalog.</p>
</div>
<div class="paragraph">
<p>Below is the direct link to the catalog item you need to order. The Arcade will guide you through the ordering process.</p>
</div>
<div class="paragraph">
<p>This environment automatically deploys a Llama-3.2-3b Generative AI model to enable LlamaStack features in OpenShift AI. Please note that this automatically consumes GPU resources on the 4x and 8x instance types.</p>
</div>
<div class="paragraph">
<p>If you do not intend to use LlamaStack, select smaller instance types to save on costs,  stopping or removing the Llama-3.2-3b model to free up the GPU.</p>
</div>
<div class="paragraph">
<p>Selecting the g6.12xlarge instance will provision a single node with four 24GB GPUs, enabling the deployment of large models and intra-GPU scenarios.</p>
</div>
<div class="paragraph">
<p><a href="https://catalog.demo.redhat.com/catalog?item=babylon-catalog-prod/published.openshift-ai-v3.prod&amp;utm_source=webapp&amp;utm_medium=share-link" target="blank">Red Hat OpenShift AI 3.0</a></p>
</div>
<iframe src="https://demo.arcade.software/w6HfuWHzkJ0rx9yXCzQk?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_2_initial_cluster_login_validation"><a class="anchor" href="#_2_initial_cluster_login_validation"></a>2. Initial Cluster Login &amp; Validation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This interactive experience provides a brief tour of the new menu options in OpenShift AI 3.0. In addition, we review the now-mandatory Hardware Profiles section, where predefined instance sizes can be configured to manage compute resources.</p>
</div>
<div class="paragraph">
<p>Next, we deploy a Granite model from the AI Hub Catalog to validate that the model inference components are fully available for standard vLLM deployments.</p>
</div>
<iframe src="https://demo.arcade.software/GfqvGnWw1uQdlBhkLhy9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
<div class="sect1">
<h2 id="_3_openshift_console_configuiration_and_settings"><a class="anchor" href="#_3_openshift_console_configuiration_and_settings"></a>3. OpenShift Console Configuiration and Settings</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This interactive experience begins with your first hands-on steps: logging into the OpenShift Console to validate that the necessary Operators and OCP versions are ready to support llm-d.</p>
</div>
<div class="paragraph">
<p>We will first review the installed Operators from the ecosystem menu. Following this, we will install the Leader Worker Set Operator, which is required for llm-d to manage a fleet of pods for distributed inference.</p>
</div>
<div class="paragraph">
<p>Next, we will verify the hardware to ensure sufficient capacity is available and confirm that the OpenShift version is at least 4.19.</p>
</div>
<div class="paragraph">
<p>Finally, we will double-check that all operators have installed successfully before proceeding to the next segment.</p>
</div>
<iframe src="https://demo.arcade.software/S1dHIjii1Vjkqi0phdSd?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
<div class="sect1">
<h2 id="_4_deploying_an_model_using_llm_d_via_the_openshift_ai_console"><a class="anchor" href="#_4_deploying_an_model_using_llm_d_via_the_openshift_ai_console"></a>4. Deploying an Model using llm-d via the OpenShift AI Console</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This interactive experience begins in the OpenShift AI console and quickly proceeds to deploy a Granite model using the llm-d runtime.</p>
</div>
<div class="paragraph">
<p>We start by selecting Deploy a model from the project menu. Next, we utilize an existing data connection to provide the Granite AI model files.</p>
</div>
<div class="paragraph">
<p>Proceed by entering the model name and selecting the runtime. For the Hardware Profile, ensure you select the profile configured for GPU usage.</p>
</div>
<div class="paragraph">
<p>Finally, deselect the option to use token authentication. In this specific environment, keeping token authentication enabled may prevent the deployment from completing successfully.</p>
</div>
<div class="paragraph">
<p>Optionally, you can choose to make this model available in the Playground (also known as the OpenShift AI Studio).</p>
</div>
<iframe src="https://demo.arcade.software/WDbYI8agiNhgCIfYPLp9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
<div class="sect1">
<h2 id="_5_view_the_deployment_progress_in_openshift"><a class="anchor" href="#_5_view_the_deployment_progress_in_openshift"></a>5. View the Deployment Progress in OpenShift</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this interactive experience, we return to the OpenShift Console to monitor the deployment of our llm-d inference service.</p>
</div>
<div class="paragraph">
<p>First, we will verify the deployed Pods. We should expect to see two specific pods: the intelligent scheduler and the vLLM-powered inference Pod responsible for running the model.</p>
</div>
<div class="paragraph">
<p>Next, we will examine the Topology view, logs, and events to track the progress of our deployment.</p>
</div>
<iframe src="https://demo.arcade.software/z3WQEOr7Vd1QrGmxVRQf?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"  width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
<div class="sect1">
<h2 id="_6_validating_the_deployment"><a class="anchor" href="#_6_validating_the_deployment"></a>6. Validating the Deployment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this final interactive experience, we execute a query to confirm that our Inference Server is successfully deployed, responding to requests, and performing as expected.</p>
</div>
<div class="paragraph">
<p>We will start by using a Workbench to launch a Jupyter Notebook environment. Next, we will clone a Git repository containing the notebook that provides the specific instructions for querying our model.</p>
</div>
<div class="paragraph">
<p>After running the initial cell, you must customize the Model Name and the Inference Endpoint. Be sure to set the API Key to a null value, as we disabled token authentication during the deployment phase.</p>
</div>
<div class="paragraph">
<p>Finally, we will send a couple of distinct queries to validate the model&#8217;s functionality and response quality.</p>
</div>
<iframe src="https://demo.arcade.software/cKkaV0pgSsSJnRM0wRsj?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="benefits.html">llm-d Use Cases and Core Benefits</a></span>
  <span class="next"><a href="../appendix/appendix.html">appendix</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
