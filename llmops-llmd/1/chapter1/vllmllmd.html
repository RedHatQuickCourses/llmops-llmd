<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The Distributed Inference Stack: vLLM &amp; llm-d :: Maximize Your GPU ROI, Scaling LLM Inference llm-d</title>
    <link rel="prev" href="../index.html">
    <link rel="next" href="llmdarch.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="vllmllmd.html">The Distributed Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="llmdarch.html">System Mechanics: Building the Foundation &amp; Request Lifecycle</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="simulator.html">Mission Simulator: Distributed Inference Operations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="labguide.html">Lab Mission: Deploying Distributed Inference</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="summary.html">Mission Debrief: Maximizing ROI &amp; Next Steps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/well-lit-paths.html">Mission Data: Reference &amp; Glossary</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/trbleshoot.html">Mission Recovery: Troubleshooting &amp; FAQs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive: The Physics of Latency (KV Cache)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Maximize Your GPU ROI, Scaling LLM Inference llm-d</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></li>
    <li><a href="vllmllmd.html">The Distributed Inference Stack: vLLM &amp; llm-d</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">The Distributed Inference Stack: vLLM &amp; llm-d</h1>
<div id="preamble">
<div class="sectionbody">
<div class="imageblock text-left">
<div class="content">
<img src="https://www.redhat.com/cms/managed-files/styles/wysiwyg_full_width/s3/Standard%20Logo.png?itok=media_token" alt="Red Hat OpenShift AI" width="200">
</div>
</div>
<div class="paragraph">
<p><strong>Role:</strong> System Architect
<strong>Technical Objective:</strong> Understand the "Stacked Architecture" that decouples orchestration (Control Plane) from execution (Inference Engine).</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_industrializing_generative_ai"><a class="anchor" href="#_industrializing_generative_ai"></a>Industrializing Generative AI</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To move from "Experiment" to "Enterprise Service," we must stop treating model serving as a single, monolithic pod.  Industrial-grade inference requires two distinct layers of optimization:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>A High-Performance Engine:</strong> To manage raw speed and memory at the node level.</p>
</li>
<li>
<p><strong>An Intelligent Control Plane:</strong> To manage resource efficiency and routing across the entire cluster.
Red Hat OpenShift AI combines the industry-standard <strong>vLLM</strong> engine with the Kubernetes-native <strong>llm-d</strong> framework to create this unified, production-grade stack.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_stack_architecture"><a class="anchor" href="#_the_stack_architecture"></a>The Stack Architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following architectural model visualizes the functional separation of the inference stack.</p>
</div>
<div class="sect2">
<h3 id="_layer_1_the_control_plane_llm_drole_orchestration_routing"><a class="anchor" href="#_layer_1_the_control_plane_llm_drole_orchestration_routing"></a>Layer 1: The Control Plane (llm-d)<strong>Role:</strong> Orchestration &amp; Routing</h3>
<div class="paragraph">
<p>At the top of the stack is <strong>llm-d</strong>, a Kubernetes-native framework that transforms monolithic deployments into intelligent microservices. It acts as the "Traffic Cop" for your compute.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Intelligent Scheduling:</strong> Unlike standard Kubernetes round-robin routing, <code>llm-d</code> utilizes live metadata to route requests to the "warmest" nodes (KV Cache Affinity).</p>
</li>
<li>
<p><strong>Disaggregation:</strong> It splits the compute-heavy "Prefill" phase from the latency-sensitive "Decode" phase into separate services, optimizing hardware utilization.</p>
</li>
<li>
<p><strong>MoE Parallelism:</strong> It manages the complex inter-node communication required for massive Mixture-of-Expert (MoE) models that exceed single-node capacity.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_layer_2_the_service_layer_vllm_role_high_throughput_execution"><a class="anchor" href="#_layer_2_the_service_layer_vllm_role_high_throughput_execution"></a>Layer 2: The Service Layer (vLLM) <strong>Role:</strong> High-Throughput Execution</h3>
<div class="paragraph">
<p>The middle layer is powered by <strong>vLLM</strong>, the state-of-the-art open-source engine. In this stack, vLLM is the worker engine running inside the pods managed by <code>llm-d</code>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>PagedAttention:</strong> Manages KV cache memory in fixed-size pages (similar to OS virtual memory), eliminating fragmentation and enabling large context windows.</p>
</li>
<li>
<p><strong>Continuous Batching:</strong> Processes multiple queries together dynamically to maximize GPU saturation.</p>
</li>
<li>
<p><strong>Speculative Decoding:</strong> Predicts future tokens in parallel to speed up the generation phase.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_layer_3_the_hardware_layerrole_base_compute_resources"><a class="anchor" href="#_layer_3_the_hardware_layerrole_base_compute_resources"></a>Layer 3: The Hardware Layer<strong>Role:</strong> Base Compute Resources</h3>
<div class="paragraph">
<p>At the foundation are the physical resources (e.g., NVIDIA H100, A100). The software stack ensures these expensive assets are never wasted:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>vLLM</strong> ensures the GPU is not idle during execution cycles.</p>
</li>
<li>
<p><strong>llm-d</strong> ensures requests are routed to the right GPU at the right time.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">The Logistics Analogy</div>
<div class="paragraph">
<p>Think of running an LLM at scale like managing a global logistics network:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>llm-d</strong> is the <strong>Air Traffic Control Tower</strong>, optimizing routes and managing the fleet.</p>
</li>
<li>
<p><strong>vLLM</strong> is the <strong>Jet Engine</strong> on every plane, ensuring maximum speed and efficiency for the individual unit.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></span>
  <span class="next"><a href="llmdarch.html">System Mechanics: Building the Foundation &amp; Request Lifecycle</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
