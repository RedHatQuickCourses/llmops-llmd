<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Taxonomy to Know :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="prev" href="appendix.html">
    <link rel="next" href="kvcache.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter3/redhatai.html">AI Factory on Red Hat AI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter3/vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter3/section1.html">LLM-D Use Cases and Core Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter3/section2.html">LLM-D Architecture Deep Dive: Building the Foundation</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter3/section6.html">Lab Demo: llm-d Deployment Guide Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="appendix.html">appendix</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="trbleshoot.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="appendix.html">appendix</a></li>
    <li><a href="index.html">Taxonomy to Know</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Taxonomy to Know</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Before diving into the architecture, here is an optional guide to the key terms and resources. Use it to learn the language or simply as a quick refresher on what all the technical terms mean.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_core_taxonomy_speaking_the_language"><a class="anchor" href="#_core_taxonomy_speaking_the_language"></a>Core Taxonomy: Speaking the Language</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Understand these key terms to get the most out of the technical deep dive and discussions.</p>
</div>
<div class="dlist dl">
<dl>
<dt>OpenShift AI</dt>
<dd>
<p>Red Hat&#8217;s end-to-end platform for building, deploying, and managing AI/ML models at scale on the hybrid cloud.</p>
</dd>
<dt>Distributed Inference (LLM-D)</dt>
<dd>
<p>The feature within OpenShift AI (powered by open-source LLM-D) that orchestrates inference across multiple servers for scalability and efficiency.</p>
</dd>
<dt>vLLM</dt>
<dd>
<p>The high-performance inference engine used by OpenShift AI. It&#8217;s known for its efficiency, but LLM-D is what scales it across the cluster.</p>
</dd>
<dt>Inference Gateway (EPP)</dt>
<dd>
<p>The "brain" or "smart router" that intercepts all requests and decides which pod is the best one to handle it (using the Endpoint Picker Protocol).</p>
</dd>
<dt>Prefill</dt>
<dd>
<p>The first phase of inference: processing the user&#8217;s input prompt. This is a compute-heavy (parallel) operation and often the main bottleneck.</p>
</dd>
<dt>Decode</dt>
<dd>
<p>The second phase of inference: generating the response token by token. This is a memory-bandwidth-heavy (sequential) operation.</p>
</dd>
<dt>KV Cache</dt>
<dd>
<p>The model&#8217;s "short-term memory" created during Prefill. It lives in expensive GPU VRAM. Reusing this cache is the primary goal of smart scheduling.</p>
</dd>
<dt>Cache-Aware Routing</dt>
<dd>
<p>The scheduler&#8217;s "smart" ability to send a request to a pod that already has the KV Cache for that conversation, skipping the expensive Prefill step.</p>
</dd>
<dt>TTFT (Time To First Token)</dt>
<dd>
<p>A key metric for user experience. How long does the user wait from pressing "Enter" to seeing the first word of the response? (Measures responsiveness).</p>
</dd>
<dt>TPOT (Time Per Output Token)</dt>
<dd>
<p>A key metric for performance. Once the response starts, how fast do the new words appear? (Measures generation speed).</p>
</dd>
<dt>SLO (Service-Level Objective)</dt>
<dd>
<p>Your performance promise. A specific, measurable goal, e.g., "99% of all requests must have a TTFT under 500ms."</p>
</dd>
<dt>TCO (Total Cost of Ownership)</dt>
<dd>
<p>The ultimate business metric. How much does it cost to run your AI factory? Efficient scheduling (less GPU waste) directly lowers your TCO.</p>
</dd>
</dl>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="appendix.html">appendix</a></span>
  <span class="next"><a href="kvcache.html">Deep Dive on KV Cache Management</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
