<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The "Well-Lit Paths" of Deployment :: Maximize Your GPU ROI, Scaling LLM Inference llm-d</title>
    <link rel="prev" href="appendix.html">
    <link rel="next" href="trbleshoot.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter1/vllmllmd.html">The Distributed Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter1/llmdarch.html">Building the Foundation &amp; Request Lifecycle</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter1/simulator.html">Simulator: Distributed Inference Operations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter1/labguide.html">Deploying Distributed Inference</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter1/summary.html">Maximizing ROI &amp; Next Steps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="well-lit-paths.html">The "Well-Lit Paths" of Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="trbleshoot.html">Troubleshooting &amp; FAQs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kvcache.html">Deep Dive: (KV Cache)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="reference.html">Reference &amp; Glossary</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Maximize Your GPU ROI, Scaling LLM Inference llm-d</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></li>
    <li><a href="appendix.html">Appendix</a></li>
    <li><a href="well-lit-paths.html">The "Well-Lit Paths" of Deployment</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">The "Well-Lit Paths" of Deployment</h1>
<div id="preamble">
<div class="sectionbody">
<div class="imageblock text-left">
<div class="content">
<img src="_images/llmd-banner.png" alt="Red Hat OpenShift AI" width="200">
</div>
</div>
<div class="paragraph">
<p><strong>Role:</strong> System Architect<br>
<strong>Technical Objective:</strong> Select the correct distributed inference topology based on model complexity and performance requirements.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_the_three_paths_of_distributed_inference"><a class="anchor" href="#_the_three_paths_of_distributed_inference"></a>The Three Paths of Distributed Inference</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In Red Hat OpenShift AI 3.0, we define three "Well-Lit Paths" for deploying <code>llm-d</code>. These represent increasing levels of complexity and capability.</p>
</div>
<div class="sect2">
<h3 id="_path_1_intelligent_inference_scheduling_the_on_ramp"><a class="anchor" href="#_path_1_intelligent_inference_scheduling_the_on_ramp"></a>Path 1: Intelligent Inference Scheduling (The "On-Ramp")</h3>
<div class="paragraph">
<p><strong>Target:</strong> Standard Models (e.g., Llama-3-8B, Mistral-7B) on Single Nodes.<br>
<strong>Key Feature:</strong> Cache-Aware Routing.</p>
</div>
<div class="paragraph">
<p>This is the flagship feature and the starting point for most teams.
* <strong>The Logic:</strong> It keeps the standard architecture (one pod = full model) but adds the "Smart Router".
* <strong>The Benefit:</strong> It stops the "Cold Start" problem. By routing requests to the GPU that holds the KV cache, it enables high responsiveness without changing the underlying model topology.
* <strong>Use Case:</strong> General-purpose chatbots and low-latency internal tools.</p>
</div>
</div>
<div class="sect2">
<h3 id="_path_2_prefilldecode_disaggregation_the_optimizer"><a class="anchor" href="#_path_2_prefilldecode_disaggregation_the_optimizer"></a>Path 2: Prefill/Decode Disaggregation (The "Optimizer")</h3>
<div class="paragraph">
<p><strong>Target:</strong> High-Throughput Production Services.<br>
<strong>Key Feature:</strong> Specialized Microservices.</p>
</div>
<div class="paragraph">
<p>This path splits the monolithic inference process into two distinct specialized roles:
* <strong>Prefill Pods (Compute-Heavy):</strong> Optimized to process the massive "Prompt" context quickly. They calculate the initial state and write it to the KV Cache.
* <strong>Decode Pods (Memory-Heavy):</strong> Optimized to generate the response token-by-token. They read from the KV Cache and stream output.
* <strong>The Benefit:</strong> Guaranteed SLOs. A massive incoming prompt (Prefill) will never block a user waiting for a simple response (Decode).</p>
</div>
</div>
<div class="sect2">
<h3 id="_path_3_wide_expert_parallelism_the_boss_level"><a class="anchor" href="#_path_3_wide_expert_parallelism_the_boss_level"></a>Path 3: Wide Expert Parallelism (The "Boss Level")</h3>
<div class="paragraph">
<p><strong>Target:</strong> Massive Models (e.g., Mixtral 8x22B, Grok-1) spanning multiple nodes.<br>
<strong>Key Feature:</strong> Multi-Node Tensor Parallelism.</p>
</div>
<div class="paragraph">
<p>Reserved for "Mixture-of-Experts" (MoE) models that are too large to fit on a single GPU or even a single node.
* <strong>The Logic:</strong> The model is sharded across multiple physical nodes. <code>llm-d</code> manages the high-speed interconnects to make them behave as one giant GPU.
* <strong>The Requirement:</strong> This path strictly requires high-performance networking (InfiniBand/RoCE) to function.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_persona_guide_who_owns_this_stack"><a class="anchor" href="#_persona_guide_who_owns_this_stack"></a>Persona Guide: Who owns this stack?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Implementing <code>llm-d</code> is a specialized task. We identify two distinct personas for this workflow.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 40%;">
<col style="width: 40%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>Persona</strong></th>
<th class="tableblock halign-left valign-top"><strong>Role &amp; Focus</strong></th>
<th class="tableblock halign-left valign-top"><strong>Required Skills</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>The Operator</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Platform/SRE Team</strong><br>
Installs, configures, and monitors the <code>llm-d</code> infrastructure.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">* Kubernetes Operators &amp; Networking
* Prometheus/Grafana Observability
* GPU Hardware Drivers</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>The User</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Data Scientist / App Dev</strong><br>
Consumes the service via API.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">* OpenAI-compatible API usage
* Prompt Engineering
* <strong>Abstracted away from infrastructure complexity.</strong></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="appendix.html">Appendix</a></span>
  <span class="next"><a href="trbleshoot.html">Troubleshooting &amp; FAQs</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
