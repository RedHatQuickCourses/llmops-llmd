<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>AI Factory on Red Hat AI :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="prev" href="../index.html">
    <link rel="next" href="vllmllmd.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="redhatai.html">AI Factory on Red Hat AI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section1.html">LLM-D Use Cases and Core Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section2.html">LLM-D Architecture Deep Dive: Building the Foundation</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section6.html">Lab Demo: llm-d Deployment Guide Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/trbleshoot.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="redhatai.html">AI Factory on Red Hat AI</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">AI Factory on Red Hat AI</h1>
<div id="preamble">
<div class="sectionbody">
<div class="imageblock text-center">
<div class="content">
<img src="_images/aifactoryadvan001.png" alt="llm-d OpenShift AI Benefits Diagram" width="700">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introduction_to_the_ai_factory_on_red_hat_ai"><a class="anchor" href="#_introduction_to_the_ai_factory_on_red_hat_ai"></a>Introduction to the AI Factory on Red Hat AI</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This segment centers on transforming ad-hoc AI experiments into an enterprise-ready operation, often referred to as the "AI Factory". The need for this shift arises because 95% of AI projects are failing to deliver ROI by remaining manual, ad-hoc, and disconnected from necessary enterprise-ready capabilities.</p>
</div>
<div class="paragraph">
<p>The AI Factory is not a product but an operational framework that industrializes the production of intelligence. It requires the convergence of three pillars: People (new roles), Process (DataOps, MLOps, ModelOps), and Platform. Red Hat AI 3 acts as that platformâ€”the unifying environment that makes the factory possible.</p>
</div>
<div class="sect2">
<h3 id="_the_structure_of_the_ai_factory_built_on_red_hat_ai_includes"><a class="anchor" href="#_the_structure_of_the_ai_factory_built_on_red_hat_ai_includes"></a>The structure of the AI Factory built on Red Hat AI includes:</h3>
<div class="sect3">
<h4 id="_the_control_plane_the_front_office"><a class="anchor" href="#_the_control_plane_the_front_office"></a>The Control Plane (The Front Office):</h4>
<div class="paragraph">
<p>This is the primary user interface where teams interact with the platform. Components include the AI Hub (for Platform Admins to govern the factory) and the Gen AI Studio (for developers to build and experiment with Agentic AI solutions).</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_assembly_line_the_core_process"><a class="anchor" href="#_the_assembly_line_the_core_process"></a>The Assembly Line (The Core Process):</h4>
<div class="paragraph">
<p>This standardized workflow moves raw models and data through five key stages to build intelligent applications: Selection, adding Context (via RAG), Aligning models with business skills, Developing autonomous agents (using Llama Stack), and finally Serving it at scale.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_platform_engineering_layer_factory_operations"><a class="anchor" href="#_the_platform_engineering_layer_factory_operations"></a>The Platform Engineering Layer (Factory Operations):</h4>
<div class="paragraph">
<p>This layer provides the cross-cutting services necessary for security and cost efficiency. Key tools here include Guardrails (to ensure interactions are safe and compliant) and llm-d.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_foundation_open_source_pluggable"><a class="anchor" href="#_the_foundation_open_source_pluggable"></a>The Foundation (Open Source &amp; Pluggable):</h4>
<div class="paragraph">
<p>This layer ensures that the platform is not locked into a single supplier. Red Hat AI provides the abstraction layer, allowing users to plug in their models, their enterprise data, and their existing tools. This supports the promise of "Any Model, Any Hardware, Any Cloud".</p>
</div>
<hr>
<iframe src="https://demo.arcade.software/0mMZIG9NEbtjuprfGija?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_role_of_llm_d_in_the_ai_factory"><a class="anchor" href="#_the_role_of_llm_d_in_the_ai_factory"></a>The Role of llm-d in the AI Factory</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Within the Platform Engineering layer, llm-d is positioned as the industry&#8217;s first Kubernetes-native inference scheduler. It acts as the "traffic cop" for compute, intelligently routing requests to maximize GPU usage, which solves the "cost of inference" problem and turns a cost center into a streamlined, efficient Enterprise Service.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></span>
  <span class="next"><a href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
