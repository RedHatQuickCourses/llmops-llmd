<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>llm-d Architecture Deep Dive: Building the Foundation :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="prev" href="vllmllmd.html">
    <link rel="next" href="benefits.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="redhatai.html">AI Factory on Red Hat AI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="llmdarch.html">llm-d Architecture Deep Dive: Building the Foundation</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="benefits.html">llm-d Use Cases and Core Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section6.html">Lab Demo: llm-d Deployment Guide Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/trbleshoot.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="llmdarch.html">llm-d Architecture Deep Dive: Building the Foundation</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">llm-d Architecture Deep Dive: Building the Foundation</h1>
<div class="sect1">
<h2 id="_the_foundation_the_prerequisites"><a class="anchor" href="#_the_foundation_the_prerequisites"></a>The Foundation (The Prerequisites)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before you deploy a single model, the right foundation must be in place. Failure to meet these component requirements is the number one cause of implementation issues.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/llm-dstack.png" alt="llm-d framework on OpenShift AI" width="700">
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Component Requirements for Distributed Inference</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Component</th>
<th class="tableblock halign-left valign-top">Requirement</th>
<th class="tableblock halign-left valign-top">The "Why"</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Kubernetes / OpenShift</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift 4.19.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This is a modern stack that relies on up-to-date Kubernetes features and includes the <strong>Gateway API</strong>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Gateway API</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A compliant Gateway API implementation, such as the one provided by OpenShift Service Mesh (Istio) or kGateway.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This component serves as the <strong>"front door"</strong> for all traffic and is the required integration point for the smart scheduler.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>GPU Hardware</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">NVIDIA Data Center GPUs (e.g., H100, A100, L40S).</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This stack is built for high-performance, and the underlying <strong>vLLM</strong> engine is optimized for these specific architectures, not consumer cards.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>GPU Operators</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">NVIDIA GPU Operator &amp; Node Feature Discovery (NFD) Operator.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The GPU Operator manages drivers, and NFD advertises GPU features to the Kubernetes scheduler.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Networking (CRITICAL)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High-Speed, Low-Latency Fabric, such as <strong>InfiniBand or RoCE (RDMA)</strong>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required for <strong>"East-West" traffic</strong> between nodes, which is essential for advanced features like P/D Disaggregation and Mixture-of-Experts (MoE). A standard 10GbE TCP/IP network will be a major bottleneck and is not supported for high-performance paths.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="title">A Note on Networking</div>
Do not underestimate the networking requirement. A standard 10GbE TCP/IP network will not be fast enough for high-performance "East-West" cache sharing. A slow network will make your distributed system slower than a single pod.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_core_components"><a class="anchor" href="#_the_core_components"></a>The Core Components</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This architecture diagram demonstrates the required operators, and the routing logic that takes place when deploying <code>llm-d</code> powered Generative AI Models into OpenShift AI as a set of controllers and routing logic.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/high_level_llmd_diagram.jpg" alt="llm-d OpenShift AI Architecture Diagram showing Gateway" width="Scheduler" height="and vLLM Pods">
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Core llm-d Components and Roles</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Component</th>
<th class="tableblock halign-left valign-top">Role in the System (What It Does)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Gateway API + Inference Extension</strong> (e.g., Envoy Proxy)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>The "Smart Router"</strong> and single entry point for all users, living in the <code>openshift-ingress</code> namespace. It intercepts the user&#8217;s HTTPROUTE request and uses the inference extension to ask the "brain" (the scheduler) where to send it.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Intelligent Inference Scheduler (EPP)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>The "Intelligence"</strong> of <code>llm-d</code>. It is a plug-in to the Gateway that uses the Endpoint Picker Protocol (EPP). It only makes decisionsâ€”it does not move data. It maintains a real-time view of all vLLM pods and runs its "Filter-and-Score" logic to pick the best destination.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>vLLM Pods (Prefill/Decode)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>The "Workers"</strong>. These are vLLM engines running in the user namespace, deployed by the ModelService CRD. In advanced "Well-Lit Paths," they are split into two separate pools:
*   <strong>Prefill:</strong> Optimized for the compute-heavy task of processing the prompt.
*   <strong>Decode:</strong> Optimized for the memory-fast task of generating tokens.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>New ModelService CRD</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>The "Workload Definition"</strong>. This is the simple YAML file you, the user, create to declare what model you want to serve (e.g., "I want to serve Llama-3-8B").</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>KServe "Next GenAI" Controller (RHOAI Operator)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>The "Orchestrator"</strong>. Part of the Red Hat OpenShift AI Operator v2.25+, it watches for a ModelService CRD creation. When it sees one, it builds all the complex underlying pieces, including the Prefill Deployment, the Decode Deployment, Kubernetes Services, and the HTTPROUTE rules for the Gateway.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Dependant Operators</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Supporting Infrastructure</strong>. The Cert Manager Operator, Arthorino Operator, and the <strong>Leader-Worker-Set Operator</strong> must be installed to deploy resources using distributed inference with <code>llm-d</code>.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_the_data_flow_the_request_lifecycle"><a class="anchor" href="#_the_data_flow_the_request_lifecycle"></a>The Data Flow (The "Request Lifecycle")</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Understanding this six-step flow, detailing how the core components interact, is critical for successful deployment and troubleshooting.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/dataflow.png" alt="The Request Data flow Distributed Inference with llm-d" width="700">
</div>
<div class="title">Figure 1. The Request Data flow Distributed Inference with llm-d</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Request In:</strong> A user sends an <strong>OpenAI-compatible request</strong> (e.g., <code>/v1/chat/completions</code>) to the HTTPROUTE defined for the model.</p>
</li>
<li>
<p><strong>Gateway Intercepts:</strong> The Gateway API (Envoy), located in the <code>openshift-ingress</code> namespace, receives the request.</p>
</li>
<li>
<p><strong>The "Brain" is Called:</strong> The Gateway API inference extension forwards the request metadata to the Inference Scheduler (EPP), essentially asking: "Where should this new request go?".</p>
</li>
<li>
<p><strong>Scheduler Decides (Filter &amp; Score):</strong> The Scheduler runs its logic:</p>
<div class="ulist">
<ul>
<li>
<p><strong>Filter:</strong> It gathers a list of all vLLM pods for the requested model and filters out any that are unhealthy, overloaded, or incompatible.</p>
</li>
<li>
<p><strong>Score:</strong> It scores the remaining, healthy pods based on telemetry. It checks which pod has the shortest queue and, most importantly, <strong>if any pod already holds the KV Cache for the conversation (KV Cache Affinity)</strong>.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Optimal Pod Selected:</strong> The Scheduler determines the best pod and routes accordingly:</p>
<div class="ulist">
<ul>
<li>
<p><strong>Cache Hit (Fast Path):</strong> If it finds a Decode pod with the KV Cache and a short queue (e.g., <code>decode-pod-7</code>), it tells the Gateway, "Send it there".</p>
</li>
<li>
<p><strong>Cache Miss (Slower Path):</strong> If no pod has the required cache, it tells the Gateway, "Send it to a Prefill pod first (the least busy one). That Prefill pod will then process the prompt, write the new KV Cache, and internally hand the request off to a Decode pod for generation".</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Routing &amp; Response:</strong> The Gateway forwards the actual request to the chosen pod. The pod generates the response and streams it back to the user through the Gateway.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_observability_proving_it_works"><a class="anchor" href="#_observability_proving_it_works"></a>Observability (Proving It Works)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As an implementer, you must prove the platform&#8217;s value and economic viability. The <code>llm-d</code> stack is built for detailed observability, providing the "Golden Signals" necessary to connect metrics to business value.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 3. Golden Signals for llm-d Observability</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Business Goal</th>
<th class="tableblock halign-left valign-top">Metric</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>To Prove Performance (SLOs)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>vllm_llmd_time_to_first_token_seconds</code> (TTFT)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This is the <strong>"responsiveness" metric</strong>. Your goal is to keep this value low and stable to meet user expectations.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>To Prove Performance (SLOs)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>vllm_llmd_time_per_output_token_seconds</code> (TPOT)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This is the <strong>"generation speed"</strong> metric.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>To Prove TCO (Cost Savings)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>vllm_llmd_kv_cache_hit_rate</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Your #1 TCO METRIC</strong>. A high hit rate (e.g., 90%) means you are skipping the expensive Prefill step 90% of the time, representing a direct measure of your return on investment (ROI).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>To Prove TCO (Cost Savings)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>vllm_llmd_gpu_utilization_seconds</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Proves that your expensive GPUs are being used effectively, rather than sitting idle.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Monitoring Setup</div>
These metrics are scraped by Prometheus and should be viewed in Grafana, allowing implementers to connect the technical performance directly to the "Why Buy?" value propositions.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs"></code></pre>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a></span>
  <span class="next"><a href="benefits.html">llm-d Use Cases and Core Benefits</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
