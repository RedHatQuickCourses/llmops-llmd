<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Deploying Your First Distributed Inference Service :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="prev" href="section4.html">
    <link rel="next" href="section6.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">RDHP Lab: Environment Setup</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section1.html">LLM-D Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section2.html">Use Case Identification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section3.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section4.html">Distributed Inference Architecture</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="section5.html">Lab: Deploying Your First Distributed Inference Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section6.html">Hands-on Lab: Deploying Distributed Inference</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Taxonomy to Know</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="section5.html">Lab: Deploying Your First Distributed Inference Service</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Deploying Your First Distributed Inference Service</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the hands-on lab for OpenShift AI&#8217;s Distributed Inference.</p>
</div>
<div class="paragraph">
<p>In this guide, you will deploy your first distributed AI inference service on OpenShift AI. You will follow the "well-lit path" for Inference Scheduling (powered by LLM-D) to deploy a highly efficient, scalable, and enterprise-grade service.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_goal"><a class="anchor" href="#_lab_goal"></a>Lab Goal</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By the end of this lab, you will have:</p>
</div>
<div class="paragraph">
<p>Installed the OpenShift AI Gateway (kgateway).</p>
</div>
<div class="paragraph">
<p>Deployed a vLLM model service with disaggregated (separate) Prefill and Decode pods.</p>
</div>
<div class="paragraph">
<p>Validated that your deployment is running and ready for inference.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_why_buy_this_lab"><a class="anchor" href="#_the_why_buy_this_lab"></a>The "Why Buy?" This Lab</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This lab proves the core value of OpenShift AI&#8217;s distributed inference:</p>
</div>
<div class="paragraph">
<p>Performance: You will deploy a stack that intelligently separates large "Prefill" jobs from small "Decode" jobs. This eliminates queue-blocking and is the key to guaranteeing your performance SLOs.</p>
</div>
<div class="paragraph">
<p>Lower TCO: You stop wasting expensive GPU cycles. This architecture is designed to maximize GPU utilization, so you get the full value from your hardware.</p>
</div>
<div class="paragraph">
<p>Security: The entire stack runs on your private OpenShift cluster, in your hybrid cloud environment, ensuring your proprietary data never leaves your control.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_before_you_begin_prerequisites"><a class="anchor" href="#_before_you_begin_prerequisites"></a>Before You Begin: Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This lab guide assumes you have ALREADY COMPLETED the "OpenShift AI: Lab Environment Setup" guide.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Lab Environment Check</div>
Your OpenShift environment must have the following components running:
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>An OpenShift 4.17+ cluster.</p>
</li>
<li>
<p>At least two GPU-enabled nodes (e.g., L40S or A100).</p>
</li>
<li>
<p>The Node Feature Discovery (NFD) Operator.</p>
</li>
<li>
<p>The NVIDIA GPU Operator.</p>
</li>
</ul>
</div>
<hr>
<div class="paragraph">
<p>If your environment is ready, let&#8217;s begin.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_1_client_and_namespace_setup"><a class="anchor" href="#_part_1_client_and_namespace_setup"></a>Part 1: Client and Namespace Setup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, we need to prepare your client (your local machine or bastion) and the OpenShift project where we&#8217;ll work.</p>
</div>
<div class="sect2">
<h3 id="_1_set_namespace_variable"><a class="anchor" href="#_1_set_namespace_variable"></a>1. Set Namespace Variable</h3>
<div class="paragraph">
<p>To make commands easier to copy/paste, let&#8217;s set a namespace variable and create that project.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">export NAMESPACE="llm-d-lab"
oc new-project $NAMESPACE</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_get_the_required_client_tools"><a class="anchor" href="#_2_get_the_required_client_tools"></a>2. Get the Required Client Tools</h3>
<div class="paragraph">
<p>This lab requires kubectl, helm, yq, and git. This repository contains a helper script to install them.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This guide assumes you are running these commands from a terminal within the cloned course repository. From the root of this repository
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">prereq/client-setup/install-deps.sh</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_3_create_your_huggingface_token_secret"><a class="anchor" href="#_3_create_your_huggingface_token_secret"></a>3. Create Your HuggingFace Token Secret</h3>
<div class="paragraph">
<p>To download models like Llama 3, the service needs a HuggingFace (HF) token.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If you don&#8217;t have one, go to your HuggingFace Profile, navigate to Settings &#8594; Access Tokens, and create a new token with "read" permissions.</p>
</li>
<li>
<p>Create the Kubernetes secret in your project (replace YOUR_TOKEN_HERE with your new token).</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc create secret generic hf-token
--from-literal=token=YOUR_TOKEN_HERE
--namespace $NAMESPACE</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_2_deploy_the_openshift_ai_inference_stack"><a class="anchor" href="#_part_2_deploy_the_openshift_ai_inference_stack"></a>Part 2: Deploy the OpenShift AI Inference Stack</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now we will deploy the two main components using the Helm charts included in this repository.</p>
</div>
<div class="sect2">
<h3 id="_1_install_the_gateway_provider"><a class="anchor" href="#_1_install_the_gateway_provider"></a>1. Install the Gateway Provider</h3>
<div class="paragraph">
<p>First, we deploy the kgateway, which will act as our smart router.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">helm install llm-d-gateway ./charts/kgateway
-n $NAMESPACE
--wait</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_deploy_the_inference_scheduling_service"><a class="anchor" href="#_2_deploy_the_inference_scheduling_service"></a>2. Deploy the "Inference Scheduling" Service</h3>
<div class="paragraph">
<p>Next, we deploy the core of the lab: the LLM-D controller and the vLLM model service. This "Inference Scheduling" path is the recommended "well-lit path" for a production-ready deployment.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">helm install llm-d-vllm-deployment ./charts/llm-d-vllm-deployment
-n $NAMESPACE
-f ./charts/llm-d-vllm-deployment/values.yaml
--wait</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">What did this Helm chart do?</div>
This single command automatically:
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Installed the llm-d-controller-manager.</p>
</div>
<div class="paragraph">
<p>Defined the ModelService Custom Resource Definition (CRD).</p>
</div>
<div class="paragraph">
<p>Created a ModelService resource for llama-3-8b-chat-hf.</p>
</div>
<div class="paragraph">
<p>Deployed two separate sets of pods: one for Prefill and one for Decode.</p>
</div>
<div class="paragraph">
<p>Configured the Gateway to route traffic to them.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_3_validation"><a class="anchor" href="#_part_3_validation"></a>Part 3: Validation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s verify that all components are running.</p>
</div>
<div class="sect2">
<h3 id="_1_check_your_helm_releases"><a class="anchor" href="#_1_check_your_helm_releases"></a>1. Check your Helm releases</h3>
<div class="paragraph">
<p>You should see both charts listed as "deployed".</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">helm list -n $NAMESPACE</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Expected Output</div>
<div class="content">
<pre>NAME                    NAMESPACE   REVISION    UPDATED                                 STATUS      CHART                       APP VERSION
llm-d-gateway llm-d-lab   1           2025-10-25 12:00:00.000 -0500 CDT    deployed kgateway-0.1.0              0.1.0
llm-d-vllm-deployment llm-d-lab   1           2025-10-25 12:05:00.000 -0500 CDT    deployed llm-d-vllm-deployment-0.1.0 0.1.0</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_check_your_pods"><a class="anchor" href="#_2_check_your_pods"></a>2. Check your pods</h3>
<div class="paragraph">
<p>You should see pods for the controller, the gateway, and the separate prefill/decode model services.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pods -n $NAMESPACE</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Expected Output (Pod names will vary)</div>
<div class="content">
<pre>NAME                                          READY   STATUS    RESTARTS   AGE
llm-d-controller-manager-f7f5c6f8f-abcde 2/2     Running 0          5m
llm-d-gateway-69f8c8d8b-fghij 1/1     Running 0          7m
llama-3-8b-chat-hf-decode-7d7c9f8b8-klmno 1/1     Running 0          5m
llama-3-8b-chat-hf-decode-7d7c9f8b8-pqrst 1/1     Running 0          5m
llama-3-8b-chat-hf-prefill-6b6c8d8b8-uvwxyz 1/1     Running 0          5m</pre>
</div>
</div>
<div class="paragraph">
<p>If you see all these pods in a "Running" state, your deployment is successful!</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_4_next_steps"><a class="anchor" href="#_part_4_next_steps"></a>Part 4: Next Steps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Your AI factory is running. Now it&#8217;s time to use it.</p>
</div>
<div class="sect2">
<h3 id="_1_make_your_first_inference_request"><a class="anchor" href="#_1_make_your_first_inference_request"></a>1. Make Your First Inference Request</h3>
<div class="paragraph">
<p>Your service is running and exposed via the Gateway. To learn how to format your request and send it to your new model, proceed to the next guide:</p>
</div>
<div class="paragraph">
<p><a href="#llm-d_inference_guide.adoc" class="xref unresolved button">Next Lab: Running Your First Inference Request</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_2_optional_proving_the_value_observability"><a class="anchor" href="#_2_optional_proving_the_value_observability"></a>2. (Optional) Proving the Value: Observability</h3>
<div class="paragraph">
<p>The "Why Buy?" of this stack is its efficiency. The Helm charts automatically create PodMonitor resources to scrape metrics for Prometheus.</p>
</div>
<div class="paragraph">
<p>In the OpenShift Console, you can go to Observe &gt; Dashboards and use the built-in "user workload monitoring" to query metrics like:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>vllm_llmd_time_to_first_token_seconds (TTFT)</p>
</li>
<li>
<p>vllm_llmd_time_per_output_token_seconds (TPOT)</p>
</li>
<li>
<p>vllm_llmd_kv_cache_hit_rate (Your most important TCO metric!)</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_5_lab_cleanup"><a class="anchor" href="#_part_5_lab_cleanup"></a>Part 5: Lab Cleanup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When you are finished, use Helm to remove all components and then delete the project.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">Uninstall the llm-d deployment</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>helm uninstall llm-d-vllm-deployment -n $NAMESPACE</p>
</li>
<li>
<p>Uninstall the gateway</p>
</li>
<li>
<p>helm uninstall llm-d-gateway -n $NAMESPACE</p>
</li>
<li>
<p>Delete the project</p>
</li>
<li>
<p>oc delete project $NAMESPACE</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_complete"><a class="anchor" href="#_lab_complete"></a>Lab Complete</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Congratulations! You have successfully deployed, validated, and tested a distributed, scalable AI inference stack on OpenShift AI.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="section4.html">Distributed Inference Architecture</a></span>
  <span class="next"><a href="section6.html">Hands-on Lab: Deploying Distributed Inference</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
