<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Use Case Identification :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="prev" href="section1.html">
    <link rel="next" href="section4.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section1.html">LLM-D Benefits</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="section2.html">Use Case Identification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section4.html">Distributed Inference with llm-d OpenShift AI  Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section5.html">Hands-on Lab: Proving the Value of llm-d</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Taxonomy to Know</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">RDHP Lab: Environment Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="section2.html">Use Case Identification</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Use Case Identification</h1>
<div id="preamble">
<div class="sectionbody">
<div class="admonitionblock tip tada">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">The LLM-D Litmus Test</div>
<div class="paragraph">
<p>Wondering if LLM-D is your new best friend? If you nod your head to most of these, the answer is a resounding YES!</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> üè¢ You run a <strong>large-scale, high-throughput</strong> AI service for many users.</p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> üí∞ You are constantly worried about the <strong>Total Cost of Ownership (TCO)</strong> of your GPU farm.</p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> ü§ñ Your applications involve <strong>complex conversations</strong>, RAG, or AI agents.</p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> ‚ò∏Ô∏è You live and breathe <strong>Kubernetes</strong> and want a cloud-native solution.</p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> üìà You have a crack <strong>Platform or SRE team</strong> ready to level up their infrastructure.</p>
</li>
</ul>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_identifying_the_right_business_problem"><a class="anchor" href="#_identifying_the_right_business_problem"></a>Identifying the Right Business Problem</h2>
<div class="sectionbody">
<div class="paragraph">
<p>LLM-D is not just a technical tool; it&#8217;s a solution to specific, expensive business problems. See if you recognize any of these common scenarios.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Business Problem</th>
<th class="tableblock halign-left valign-top">Symptom (What You&#8217;re Seeing)</th>
<th class="tableblock halign-left valign-top">How llm-d Solves It</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Skyrocketing AI OpEx &amp; Poor ROI</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">"Our GPU budget is out of control. We keep buying more hardware, but our user capacity doesn&#8217;t double. We can&#8217;t make this project economically viable."</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>llm-d&#8217;s Intelligent Scheduling and KV Cache Management stop waste. They pack more users and more models onto the same hardware, directly lowering TCO and improving your cost-per-query.</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Inconsistent or Slow AI Applications</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">"Our users complain the chatbot is 'slow to start' (High $TTFT$) or 'stutters' during a long answer (High $TPOT$). Our RAG app times out under load."</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>llm-d&#8217;s P/D Disaggregation and Smart Scheduling are built to solve this. They eliminate hotspots and optimize the pipeline for fast, consistent responses, allowing you to meet your SLOs.</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data Sovereignty &amp; Security Mandates</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">"Our legal and compliance teams have blocked the use of third-party APIs. We have to build this in-house, but we can&#8217;t sacrifice performance."</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>llm-d runs inside your OpenShift cluster. Your proprietary data never leaves your secure, private, or hybrid cloud environment. It delivers public-cloud performance with private-cloud security.</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Inability to Run Next-Gen Models</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">"We&#8217;re stuck on 7B models. We want to use a new 8x7B (MoE) model to stay competitive, but we have no way to serve it efficiently."</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>llm-d&#8217;s Wide Expert Parallelism is specifically designed to serve massive Mixture-of-Experts (MoE) models, giving you a clear path to run next-generation AI.</p>
</div></div></td>
</tr>
</tbody>
</table>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_who_is_this_for_the_skills_you_need"><a class="anchor" href="#_who_is_this_for_the_skills_you_need"></a>Who is This For? The Skills You Need</h2>
<div class="sectionbody">
<div class="paragraph">
<p>LLM-D is incredibly powerful, but it is not a "magic button." It is a solution designed for a specific set of users.</p>
</div>
<div class="sect2">
<h3 id="_persona_1_the_operator_your_platformsre_team"><a class="anchor" href="#_persona_1_the_operator_your_platformsre_team"></a>Persona 1: The Operator (Your Platform/SRE Team)</h3>
<div class="paragraph">
<p>This is the primary user who installs, configures, and manages llm-d. This team should have:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Strong Kubernetes/OpenShift Expertise: Deep understanding of Operators, networking (Gateway API), and security contexts.</p>
</li>
<li>
<p>Helm &amp; GitOps Fluency: Deployments are managed via Helm charts, ideally in a GitOps workflow.</p>
</li>
<li>
<p>Observability Skills: Comfortable with Prometheus and Grafana for monitoring GPU, network, and scheduler-level metrics.</p>
</li>
<li>
<p>Hardware Awareness: Understands GPU hardware, NVIDIA drivers, and node configurations.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_persona_2_the_user_your_data_scientists_app_developers"><a class="anchor" href="#_persona_2_the_user_your_data_scientists_app_developers"></a>Persona 2: The User (Your Data Scientists &amp; App Developers)</h3>
<div class="paragraph">
<p>This is the consumer of the llm-d service. For them, life gets simpler. They should:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Be API-Driven: They only need to know the OpenAI-compatible API endpoint provided by the platform team.</p>
</li>
<li>
<p>Focus on the Application: They no longer need to be Kubernetes experts. llm-d abstracts the deep infrastructure complexity away from them.</p>
</li>
</ul>
</div>
<hr>
<div class="admonitionblock note book">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">llm-d at a Glance: The Technical Deep Dive</div>
For architects and platform leads, here is a summary of llm-d&#8217;s core technical advantages.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_architectural_advantage"><a class="anchor" href="#_architectural_advantage"></a>Architectural Advantage</h3>
<div class="paragraph">
<p>llm-d&#8217;s primary strength is its clean, three-layered architecture, which is deeply integrated with cloud-native principles. This architecture consists of:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>An intelligent routing layer built on the Kubernetes Gateway API and its new Inference Extension, which makes dynamic, telemetry-driven scheduling decisions;</p>
</li>
<li>
<p>A distributed model serving layer utilizing the high-performance vLLM engine for core inference operations; and</p>
</li>
<li>
<p>A sophisticated, pluggable KV cache hierarchy that manages the critical state of inference requests across the cluster. This modular design allows for independent optimization and scaling of each component.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_performance_drivers"><a class="anchor" href="#_performance_drivers"></a>Performance Drivers</h3>
<div class="paragraph">
<p>The framework&#8217;s significant performance improvements are not incidental but are the direct result of implementing advanced distributed systems techniques.</p>
</div>
<div class="paragraph">
<p>Key among these are Prefill/Decode (P/D) disaggregation, which separates the distinct phases of inference onto specialized workers, and KV-cache-aware routing, which intelligently directs requests to nodes that have already computed parts of the prompt. These features, amplified by a strategic collaboration with NVIDIA&#8217;s Dynamo ecosystem for accelerated data transfer and resource planning, result in empirically demonstrated gains, including up to a 3x lower Time-To-First-Token (TTFT) and a 2x increase in throughput under SLO constraints compared to baseline deployments.</p>
</div>
</div>
<div class="sect2">
<h3 id="_competitive_positioning"><a class="anchor" href="#_competitive_positioning"></a>Competitive Positioning</h3>
<div class="paragraph">
<p>Llm-d occupies a specific and strategic niche in the MLOps ecosystem. It is best understood as a specialist solution for platform and SRE teams responsible for managing large, multi-tenant GPU clusters on Kubernetes.</p>
</div>
<div class="paragraph">
<p>This focus differentiates it from: general-purpose, multi-framework inference servers like NVIDIA Triton; application-centric, developer-focused frameworks like Ray Serve; and broader, more functionally diverse MLOps platforms like KServe. llm-d prioritizes infrastructure efficiency and TCO for LLM-specific workloads above all else.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="section1.html">LLM-D Benefits</a></span>
  <span class="next"><a href="section4.html">Distributed Inference with llm-d OpenShift AI  Architecture</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
