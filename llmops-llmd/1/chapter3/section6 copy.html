<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Hands-on Lab: Deploying Distributed Inference with OpenShift AI :: vLLM to AI Factory Scaling with LLM-D</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section1.html">LLM-D Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section2.html">Use Case Identification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section3.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section4.html">Distributed Inference Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section5.html">Lab: Deploying Your First Distributed Inference Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section6.html">Hands-on Lab: Deploying Distributed Inference</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Taxonomy to Know</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">RDHP Lab: Environment Setup</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="section6%20copy.html">Hands-on Lab: Deploying Distributed Inference with OpenShift AI</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Hands-on Lab: Deploying Distributed Inference with OpenShift AI</h1>
<div id="preamble">
<div class="sectionbody">
<div class="admonitionblock important icon-info-circle">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Lab Goal &amp; Prerequisites</div>
This lab guides you through deploying a model using OpenShift AI&#8217;s Distributed Inference feature (Technology Preview), powered by LLM-D. You will create a ModelService custom resource to deploy a pre-configured model stack.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Prerequisites:</p>
</div>
<div class="paragraph">
<p>You have completed the Lab Environment Setup guide. Your OpenShift cluster (4.17+) has GPU nodes and the required operators (NFD, NVIDIA GPU Operator) installed and running.</p>
</div>
<div class="paragraph">
<p>You have oc (OpenShift CLI) access to your cluster with permissions to create resources in a project.</p>
</div>
<div class="paragraph">
<p>You have created a Hugging Face token secret named hf-token in your target project (as detailed in the Lab Setup Guide or official documentation).</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_introduction_the_modelservice_custom_resource"><a class="anchor" href="#_introduction_the_modelservice_custom_resource"></a>Introduction: The ModelService Custom Resource</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In OpenShift AI, Distributed Inference deployments are managed declaratively using the ModelService Custom Resource Definition (CRD). Instead of manually configuring gateways, controllers, and vLLM deployments, you define the desired state in a single ModelService object, and the system handles the rest.</p>
</div>
<div class="paragraph">
<p>This lab will deploy a sample ModelService configured for the "Inference Scheduling" well-lit path, which includes intelligent routing.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_part_1_prepare_your_project"><a class="anchor" href="#_part_1_prepare_your_project"></a>Part 1: Prepare Your Project</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, ensure you are in the correct OpenShift project where you created your hf-token secret. If you haven&#8217;t created one, do so now.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a new project (if needed):</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc new-project my-llm-d-lab</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Replace my-llm-d-lab with your preferred project name.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Verify your Hugging Face token secret exists:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get secret hf-token -n my-llm-d-lab</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>If this command returns "not found," revisit the Lab Environment Setup guide or the official documentation to create the secret.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_part_2_define_and_create_the_modelservice"><a class="anchor" href="#_part_2_define_and_create_the_modelservice"></a>Part 2: Define and Create the ModelService</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now, you will define the ModelService resource in a YAML file. This example uses a small, publicly available model for simplicity.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a YAML file named modelservice-sample.yaml with the following content:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: llm.stack.gov.il/v1alpha1
kind: ModelService
metadata:
name: llama-3-8b-chat-hf-sample # Unique name for this model deployment
namespace: my-llm-d-lab        # IMPORTANT: Use your project name here
spec:
modelId: meta-llama/Meta-Llama-3-8B-Instruct # Model from Hugging Face
source:
huggingface:
secretKeyRef:
name: hf-token # Name of the secret containing your HF token
replicas: 1 # Start with one replica set (adjust later for scale)</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>(Optional: Add resource requests/limits, tolerations, etc. here if needed)</p>
</div>
<div class="admonitionblock note icon-pencil">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Key Fields:
 * metadata.name: Must be unique within the namespace.
 * metadata.namespace: Crucially, set this to your project name.
 * spec.modelId: The Hugging Face model identifier.
 * spec.source.huggingface.secretKeyRef.name: Points to your hf-token secret.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Apply the ModelService manifest to your cluster:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f modelservice-sample.yaml -n my-llm-d-lab</code></pre>
</div>
</div>
</li>
</ol>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_part_3_validate_the_deployment"><a class="anchor" href="#_part_3_validate_the_deployment"></a>Part 3: Validate the Deployment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Creating the ModelService triggers the OpenShift AI operator to provision all the necessary backend components. This process takes several minutes as container images are pulled and pods are started.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Monitor the ModelService status:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get modelservice llama-3-8b-chat-hf-sample -n my-llm-d-lab -w</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Wait until the STATUS column shows Ready or Available. Press Ctrl+C to exit the watch.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check the deployed pods:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pods -n my-llm-d-lab</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>You should see pods related to the ModelService name you chose, likely including:</p>
</div>
<div class="paragraph">
<p>Pods for the inference gateway (e.g., llm-d-gateway-&#8230;&#8203;)</p>
</div>
<div class="paragraph">
<p>Pods for the llm-d controller manager</p>
</div>
<div class="paragraph">
<p>Pods for the vLLM model deployment itself (potentially separate prefill and decode pods, depending on the default configuration)</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Find the Inference Endpoint (Route):</p>
<div class="paragraph">
<p>The ModelService automatically creates an OpenShift Route to expose the inference API.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get route llama-3-8b-chat-hf-sample -n my-llm-d-lab -o jsonpath='{.spec.host}'</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Copy this hostname. You will need it to send inference requests.</p>
</div>
<div class="paragraph">
<p>If your pods are running and you have a route hostname, the deployment is successful!</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_part_4_send_an_inference_request_basic_test"><a class="anchor" href="#_part_4_send_an_inference_request_basic_test"></a>Part 4: Send an Inference Request (Basic Test)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s send a simple request using curl to verify the endpoint is working.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Set the route hostname to an environment variable:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">export MODEL_ROUTE="your-route-hostname-here" # Paste the hostname from the previous step</code></pre>
</div>
</div>
</li>
<li>
<p>Send a sample completion request:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -k https://${MODEL_ROUTE}/v1/completions</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>-H "Content-Type: application/json"</p>
</div>
<div class="paragraph">
<p>-d '{
"model": "meta-llama/Meta-Llama-3-8B-Instruct",
"prompt": "Translate the following English text to French: 'Hello, world!'",
"max_tokens": 20,
"temperature": 0.1
}'</p>
</div>
<div class="paragraph">
<p>You should receive a JSON response containing the model&#8217;s translation.</p>
</div>
<div class="admonitionblock tip icon-check-circle">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Success!</div>
Receiving a valid JSON response confirms that your distributed inference service is up and running correctly.
</td>
</tr>
</table>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_part_5_cleanup"><a class="anchor" href="#_part_5_cleanup"></a>Part 5: Cleanup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When you are finished with the lab, remove the deployed resources.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Delete the ModelService:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc delete modelservice llama-3-8b-chat-hf-sample -n my-llm-d-lab</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>This will trigger the deletion of all associated deployments, services, and routes.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>(Optional) Delete the project:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc delete project my-llm-d-lab</code></pre>
</div>
</div>
</li>
</ol>
</div>
<hr>
<div class="admonitionblock important icon-trophy">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Lab Complete!</div>
Congratulations! You have successfully deployed, validated, and tested a model using OpenShift AI&#8217;s Distributed Inference feature. You are now ready to explore more advanced configurations
</td>
</tr>
</table>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
