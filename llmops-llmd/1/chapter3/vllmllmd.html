<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The Scalable LLM Inference Stack: vLLM &amp; llm-d :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="prev" href="redhatai.html">
    <link rel="next" href="section1.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="redhatai.html">AI Factory on Red Hat AI</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section1.html">LLM-D Use Cases and Core Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section2.html">LLM-D Architecture Deep Dive: Building the Foundation</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section6.html">Lab Demo: llm-d Deployment Guide Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/trbleshoot.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">The Scalable LLM Inference Stack: vLLM &amp; llm-d</h1>
<div class="sect1">
<h2 id="_industrializing_generative_ai"><a class="anchor" href="#_industrializing_generative_ai"></a>Industrializing Generative AI</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For enterprises seeking to industrialize Generative AI, scalable inference requires combining two critical layers of optimization: a <strong>high-performance inference engine</strong> to manage speed at the node level, and an <strong>intelligent orchestration framework</strong> to manage resource efficiency across the entire cluster.</p>
</div>
<div class="paragraph">
<p>This document outlines how Red Hat AI combines the industry-standard open-source <strong>vLLM engine</strong> with the Kubernetes-native <strong>llm-d framework</strong> to create a unified, production-grade inference stack.</p>
</div>
<div class="sect2">
<h3 id="_the_stacked_architecture"><a class="anchor" href="#_the_stacked_architecture"></a>The Stacked Architecture</h3>
<div class="paragraph">
<p>The following infographic illustrates the functional separation of the LLM inference stack. It visualizes <strong>llm-d</strong> as the external control plane managing the cluster, <strong>vLLM</strong> as the internal high-throughput execution engine, and the underlying hardware resources.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/vllmllm-d.png" alt="Aligning vLLM and llm-d Layers" width="700">
</div>
<div class="title">Figure 1. The Scalable LLM Inference Stack: vLLM &amp; llm-d</div>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_the_cluster_layer_orchestration_routing_with_llm_d"><a class="anchor" href="#_the_cluster_layer_orchestration_routing_with_llm_d"></a>The Cluster Layer (Orchestration &amp; Routing with llm-d)</h3>
<div class="paragraph">
<p>At the top of the stack is <strong>llm-d</strong>, an open-source, Kubernetes-native framework designed to orchestrate distributed AI workloads. It transforms monolithic model deployments into intelligent, modular microservices.</p>
</div>
<div class="paragraph">
<p>While vLLM handles <strong>execution</strong>, llm-d handles <strong>routing and strategy</strong>. It provides the necessary control plane to manage complex, multi-node deployments on Red Hat AI.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Key Control Plane Functions:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Intelligent Scheduling (KV Cache Aware):</strong> Unlike standard Kubernetes round-robin routing, llm-d utilizes live metadata to route requests to the "warmest" nodes, maximizing KV cache reuse and reducing latency.</p>
</li>
<li>
<p><strong>Prefill/Decode Disaggregation:</strong> llm-d can split the compute-heavy "prefill" phase from the latency-sensitive "decode" phase into separate services, optimizing hardware utilization for distinct workload profiles.</p>
</li>
<li>
<p><strong>MoE/Expert Parallelism (Multi-Node):</strong> It manages the distribution of massive Mixture of Expert (MoE) models across multiple nodes, handling the complex inter-node communication required for models that exceed single-node capacity.</p>
</li>
<li>
<p><strong>Observability and TCO Optimization:</strong> As an inference gateway, llm-d provides integrated metrics for SLO tracking and enables GPU pooling strategies to maximize tokens-per-dollar.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_the_service_layer_high_throughput_execution_with_vllm"><a class="anchor" href="#_the_service_layer_high_throughput_execution_with_vllm"></a>The Service Layer (High-Throughput Execution with vLLM)</h3>
<div class="paragraph">
<p>The middle layer is powered by <strong>vLLM</strong>, the state-of-the-art open-source engine for serving LLMs. In this stack, vLLM acts as the foundational <strong>model backend</strong> running within the pods managed by llm-d.</p>
</div>
<div class="paragraph">
<p>Assuming knowledge of vLLM&#8217;s core architecture, this layer is responsible for maximizing the throughput of every allocated GPU.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Key Engine Functions:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Efficient Memory Management:</strong> Utilizes <strong>PagedAttention</strong> to manage KV cache memory in fixed-size pages, eliminating fragmentation and enabling large context windows.</p>
</li>
<li>
<p><strong>Dynamic Request Aggregation:</strong> Applies <strong>Continuous Batching</strong> to process incoming requests as soon as compute is available, rather than waiting for static batches to fill.</p>
</li>
<li>
<p><strong>Latency Reduction:</strong> Leverages <strong>Speculative Decoding</strong> to predict multiple future tokens in parallel, significantly speeding up the generation phase.</p>
</li>
<li>
<p><strong>Intra-Node Parallelism:</strong> Native support for <strong>Tensor, Pipeline, and Data Parallelism</strong> to shard models across multiple GPUs within a single compute node.</p>
</li>
<li>
<p><strong>Standard Interface:</strong> Exposes an <strong>OpenAI-compatible API endpoint</strong>, ensuring easy integration with existing applications and easy management by llm-d.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_the_hardware_layer_base_compute_resources"><a class="anchor" href="#_the_hardware_layer_base_compute_resources"></a>The Hardware Layer (Base Compute Resources)</h3>
<div class="paragraph">
<p>At the foundation are the physical compute resources, specifically data-center grade GPUs (e.g., NVIDIA H100, A100, L40S).</p>
</div>
<div class="paragraph">
<p>The combined software stack ensures these expensive resources are utilized efficiently: vLLM ensures the GPUs are not idle during execution cycles, and llm-d ensures that requests are routed to the right GPUs at the right time.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="redhatai.html">AI Factory on Red Hat AI</a></span>
  <span class="next"><a href="section1.html">LLM-D Use Cases and Core Benefits</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
