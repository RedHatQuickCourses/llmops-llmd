<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM-D Benefits :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="prev" href="../index.html">
    <link rel="next" href="section2.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="section1.html">LLM-D Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section2.html">Use Case Identification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section4.html">Distributed Inference with llm-d OpenShift AI  Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section6.html">Lab Demo: llm-d Deployment Guide Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="section1.html">LLM-D Benefits</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">LLM-D Benefits</h1>
<div id="preamble">
<div class="sectionbody">
<div class="admonitionblock tip moneybag">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Performance, Performance, Performance!</div>
At the end of the day, it&#8217;s all about results. llm-d delivers tangible gains that you can show to your boss.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Here&#8217;s a taste of what public benchmarks have shown:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>3x LOWER ðŸ“‰ average Time-To-First-Token (TTFT). Users perceive your app as being 3x faster!</p>
</li>
<li>
<p>2x HIGHER ðŸ“ˆ queries-per-second (QPS). Serve double the users with the same hardware!</p>
</li>
<li>
<p>Massive TCO Reduction ðŸ’¸. Achieve more with less. Stop buying more GPUs and start using the ones you have more efficiently.</p>
</li>
</ul>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p>ðŸ¤” "But why should I care?"</p>
</div>
<div class="paragraph">
<p>Simple. LLMs are expensive. Every wasted GPU cycle is money down the drain. Distributed inference with llm-d is designed to plug those leaks, letting you serve more users, faster, and for a fraction of the cost. It&#8217;s not just about tech; it&#8217;s about making your AI projects economically viable.</p>
</div>
</div>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_your_ai_is_powerful_but_is_it_efficient"><a class="anchor" href="#_your_ai_is_powerful_but_is_it_efficient"></a>Your AI is Powerful, but is it Efficient ?</h2>
<div class="sectionbody">
<iframe src="https://demo.arcade.software/E4Hu9xVtbxiFrzG8NoHK?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
</div>
</div>
<div class="sect1">
<h2 id="_llm_d_well_lit_paths"><a class="anchor" href="#_llm_d_well_lit_paths"></a>LLM-D Well-Lit Paths</h2>
<div class="sectionbody">
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Unleash the Power!</div>
This is where distributed inference with llm-d truly shines. It offers a set of powerful, production-ready techniques, known as "Well-Lit Paths," designed to supercharge your inference. Let&#8217;s explore them below.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_path_1_intelligent_inference_scheduling"><a class="anchor" href="#_path_1_intelligent_inference_scheduling"></a>âš¡ Path #1: Intelligent Inference Scheduling</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This is the flagship feature of LLM-D and the essential "on-ramp" to distributed inference. While critical for large models, it adds intelligent routing to any model served with OpenShift AI using vLLM.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p><strong>Why Basic Schedulers Fail for LLMs</strong></p>
</div>
<div class="paragraph">
<p>Traditional load balancing (like round-robin) is inefficient for AI workloads due to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Stateful Nature: LLM inference is stateful, using KV caches in GPU memory that benefit from request affinity.</p>
</li>
<li>
<p>Variable Request Costs: A long RAG prompt has a totally different cost than a short chat query, making uniform distribution wasteful.</p>
</li>
<li>
<p>Resource Utilization: Basic schedulers create "hotspots" (overloaded GPUs) while others sit idle, wasting expensive hardware.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_llm_d_replaces_load_balancer_with_an_intelligent_metrics_aware_kserve_gateway"><a class="anchor" href="#_llm_d_replaces_load_balancer_with_an_intelligent_metrics_aware_kserve_gateway"></a>LLM-D replaces load balancer with an intelligent, metrics-aware KServe Gateway.</h3>
<div class="paragraph">
<p>This gateway actively monitors the real-time load, queue depth, and in-flight requests of all your model pods. It then uses a two-step "Filter-and-Score" process to find the perfect home for every single request.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>FILTER:</strong> "Who is available for this job?" (Eliminates overloaded, incompatible, or failing pods).</p>
</li>
<li>
<p><strong>SCORE:</strong> "Of the ones left, who is the best for the job?" (Scores remaining pods based on real-time load, shortest queue, and for KV cache affinity).</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_path_2_prefilldecode_pd_disaggregation"><a class="anchor" href="#_path_2_prefilldecode_pd_disaggregation"></a>ðŸš€ Path #2: Prefill/Decode (P/D) Disaggregation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This is the "microservices moment" for AI. Instead of one server doing two very different jobs, we split them up!</p>
</div>
<div class="paragraph">
<p>Before: One big, clunky server trying to do everything.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>+-------------------------------------+
| [ One GPU per Inference Server ]    |
|  - Juggling compute-heavy Prefill   |
|  - And memory-heavy Decode          |
|  - (Struggling to keep up!)         |
+-------------------------------------+</pre>
</div>
</div>
<div class="paragraph">
<p>After: A specialized team working in perfect harmony.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>+-------------------+      +--------------------+
| [ Prefill Team ]  |-----&gt;| [ Decode Team ]    |
| - A+ at compute   |      | - Masters of Memory|
| - Specialized GPUs|      | - Optimized GPUs   |
+-------------------+      +--------------------+</pre>
</div>
</div>
<div class="paragraph">
<p>The result? A massive 25-50% performance boost for large models!</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_path_3_wide_expert_parallelism"><a class="anchor" href="#_path_3_wide_expert_parallelism"></a>ðŸŒŒ Path #3: Wide Expert Parallelism</h2>
<div class="sectionbody">
<div class="admonitionblock caution stars">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
Boss Level: This is for serving gargantuan Mixture-of-Experts (MoE) models.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>LLM-D handles the brain-melting complexity of splitting these giant models across many nodes, managing the high-speed networking needed to make them work as one cohesive unit.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_path_4_kv_cache_management"><a class="anchor" href="#_path_4_kv_cache_management"></a>ðŸ§  Path #4: KV Cache Management</h2>
<div class="sectionbody">
<div class="paragraph">
<p>That "short-term memory" we talked about? It lives in super-expensive GPU RAM. The llm-d-kv-cache-manager is a genius at moving that data to cheaper, more abundant storage.</p>
</div>
<div class="paragraph">
<p>Library Analogy</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Without LLM-D: Every time you need a fact, you run to the library, find the book, read it, and put it back. Inefficient!</p>
</li>
<li>
<p>With LLM-D (North-South Caching): You keep the book on your personal desk (CPU RAM). Faster!</p>
</li>
<li>
<p>With LLM-D (East-West Caching): You put the book on a shared table where your whole team can access it instantly (Shared Redis/Storage). Blazing fast!</p>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></span>
  <span class="next"><a href="section2.html">Use Case Identification</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
