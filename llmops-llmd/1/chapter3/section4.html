<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenShift AI 3.0 Release Notes :: vLLM to AI Factory Scaling with LLM-D</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="redhatai.html">AI Factory on Red Hat AI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="llmdarch.html">llm-d Architecture Deep Dive: Building the Foundation</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="benefits.html">llm-d Use Cases and Core Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section6.html">Lab Demo: llm-d Deployment Guide Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/trbleshoot.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
    <li><a href="section4.html">OpenShift AI 3.0 Release Notes</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">OpenShift AI 3.0 Release Notes</h1>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">OpenShift AI 3.0 Summarized Release Notes for this course</div>
<div class="paragraph">
<p><strong>Supported Features in Red Hat OpenShift AI 3.0 and llm-d:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>User Interface (UI) for Deployment:</strong> OpenShift AI now includes a user interface for configuring and deploying Large Language Models (LLMs) on the <code>llm-d</code> Serving Runtime. This streamlined interface simplifies common deployment scenarios by offering essential configuration options with sensible defaults, while still allowing explicit selection of the <code>llm-d</code> runtime.</p>
</li>
<li>
<p><strong>Observability and Grafana Integration:</strong> Platform administrators can now connect observability components to Distributed Inference with <code>llm-d</code> deployments. This allows integration with self-hosted Grafana instances for monitoring inference workloads. Teams can collect and visualize Prometheus metrics from <code>llm-d</code> for performance analysis and custom dashboard creation.</p>
</li>
<li>
<p><strong>Model-as-a-Service (MaaS) Support:</strong> MaaS is currently supported only for models deployed using the Distributed Inference Server with <code>llm-d</code> runtime.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>The following capabilities are not fully supported in OpenShift AI 3.0:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Wide Expert-Parallelism multi-node: Developer Preview.</p>
</li>
<li>
<p>Wide Expert-Parallelism on Blackwell B200: Not available but can be provided as a Technology Preview.</p>
</li>
<li>
<p>Multi-node on GB200: Not supported.</p>
</li>
<li>
<p>Gateway discovery and association are not supported in the UI during model deployment in this release. Users must associate models with Gateways by applying the resource manifests directly through the API or CLI.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>llm-d Known Issues in OpenShift AI 3.0:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>While Distributed Inference Server with llm-d appears as an available option for deployment, it is not currently listed on the Serving Runtimes page under the Settings section.</p>
</li>
<li>
<p>Models deployed using llm-d initially display a <strong>Failed</strong> status on the Deployments page in the OpenShift AI dashboard, even if associated pod logs report no errors or failures; the status automatically updates to <strong>Started</strong> when the model is ready.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
