<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>vLLM to AI Factory Scaling with LLM-D :: vLLM to AI Factory Scaling with LLM-D</title>
    <link rel="next" href="chapter3/section1.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../..">vLLM to AI Factory Scaling with LLM-D</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">vLLM to AI Factory Scaling with LLM-D</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/section1.html">LLM-D Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/section2.html">Use Case Identification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/section4.html">Distributed Inference with llm-d OpenShift AI  Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/section5.html">Hands-on Lab: Proving the Value of llm-d</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="appendix/appendix.html">Taxonomy to Know</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/index.html">RDHP Lab: Environment Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM to AI Factory Scaling with LLM-D</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="index.html">vLLM to AI Factory Scaling with LLM-D</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link is-current"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">vLLM to AI Factory Scaling with LLM-D</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">vLLM to AI Factory Scaling with LLM-D</h1>
<div class="sect2">
<h3 id="_increase_performance_and_reduce_cost_with_distributed_inference"><a class="anchor" href="#_increase_performance_and_reduce_cost_with_distributed_inference"></a>Increase Performance and Reduce Cost with Distributed Inference</h3>
<hr>
<div class="sect3">
<h4 id="_this_course_is_your_guide_to_understand_the_specific_pre_packaged_deployment_patterns_provided_by_llm_d_that_solve_real_world_scaling_challenges_from_immediate_wins_to_advanced_optimization"><a class="anchor" href="#_this_course_is_your_guide_to_understand_the_specific_pre_packaged_deployment_patterns_provided_by_llm_d_that_solve_real_world_scaling_challenges_from_immediate_wins_to_advanced_optimization"></a>This course is your guide to understand the specific, pre-packaged deployment patterns provided by <strong>llm-d</strong> that solve real-world scaling challenges, from immediate wins to advanced optimization.</h4>
<hr>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Prerequisite Knowledge</div>
This course assumes you are already familiar with the basics of Kubernetes and vLLM Inference. We will be building on those core concepts.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_core_challenge_apis_vs_on_premise_ai"><a class="anchor" href="#_the_core_challenge_apis_vs_on_premise_ai"></a>The Core Challenge: APIs vs. On-Premise AI</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Generative AI is transforming industries, but as you move from experiment to production, you face a critical business decision:</p>
</div>
<div class="quoteblock">
<blockquote>
"Why should we build, manage, and pay for our own GPU infrastructure when cheap, pay-as-you-go public API tokens exist?"
</blockquote>
</div>
<div class="paragraph">
<p>A public API offers convenience, but at the cost of control. It&#8217;s a "best-effort" service with unpredictable performance, runaway costs for high-volume tasks, and a critical security blindspot: you must send your most sensitive corporate data to a third party.</p>
</div>
<div class="paragraph">
<p>This course will show you how to build a true "AI Factory" on OpenShift AI, using the power of distributed inference with llm-d to achieve what public APIs cannot.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_value_of_an_ai_factory"><a class="anchor" href="#_the_value_of_an_ai_factory"></a>The Value of an AI Factory</h2>
<div class="sectionbody">
<div class="paragraph">
<p>How OpenShift AI and Distributed Inference (powered by llm-d) give you what public APIs cannot:</p>
</div>
<div class="sect2">
<h3 id="_optimize_total_cost_of_ownership_tco"><a class="anchor" href="#_optimize_total_cost_of_ownership_tco"></a>Optimize Total Cost of Ownership (TCO)</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Stop "Paying for Idle GPUs":</strong> Learn how llm-d&#8217;s Smart Scheduling (Path 1) replaces basic, "AI-unaware" Kubernetes scheduling. You&#8217;ll see how to eliminate hotspots, balance your cluster, and finally use 100% of the expensive hardware you&#8217;ve already paid for.</p>
</li>
<li>
<p><strong>Maximize GPU Density:</strong> Discover how KV Cache Management (Path 4) offloads the model&#8217;s "memory" from expensive GPU VRAM to cheaper CPU RAM, allowing you to run more models on the same hardware.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_guarantee_performance_slos"><a class="anchor" href="#_guarantee_performance_slos"></a>Guarantee Performance &amp; SLOs</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Move from "Best Effort" to "Guaranteed":</strong> Public APIs offer no performance guarantees. Learn how llm-d&#8217;s Disaggregated Inference (Path 2) separates compute-heavy "Prefill" from memory-fast "Decode" to ensure your interactive applications are always responsive.</p>
</li>
<li>
<p><strong>Master the "Well-Lit Paths":</strong> Understand the 4 pre-packaged, Helm-automated deployment patterns that provide optimized, step-by-step solutions for scaling any model, from the simplest to the most complex.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_ensure_total_security_control"><a class="anchor" href="#_ensure_total_security_control"></a>Ensure Total Security &amp; Control</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Achieve "Sovereign AI"</strong>: The most critical value-add. Learn how this entire, high-performance stack runs inside your own OpenShift cluster. Your proprietary RAG documents, customer data, and sensitive prompts never leave your control.</p>
</li>
<li>
<p><strong>Own Your Hybrid Cloud Strategy:</strong> Deploy a secure, consistent, and performant AI inference platform wherever your data livesâ€”on-premise, in a private cloud, or at the edge.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="next"><a href="chapter3/section1.html">LLM-D Benefits</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
