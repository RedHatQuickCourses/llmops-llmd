<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Maximize Your GPU ROI: Scaling LLM Inference llm-d :: Maximize Your GPU ROI, Scaling LLM Inference llm-d</title>
    <link rel="next" href="chapter3/redhatai.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../..">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/redhatai.html">AI Factory on Red Hat AI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/vllmllmd.html">The Scalable LLM Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/llmdarch.html">llm-d Architecture Deep Dive: Building the Foundation</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/benefits.html">llm-d Use Cases and Core Benefits</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter3/section6.html">Lab Demo: llm-d Deployment Guide Arcade Experience</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="appendix/appendix.html">appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/index.html">Taxonomy to Know</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/kvcache.html">Deep Dive on KV Cache Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/trbleshoot.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Maximize Your GPU ROI, Scaling LLM Inference llm-d</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link is-current"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></li>
    <li><a href="index.html">Maximize Your GPU ROI: Scaling LLM Inference llm-d</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Maximize Your GPU ROI: Scaling LLM Inference llm-d</h1>
<div class="sect1">
<h2 id="_welcome"><a class="anchor" href="#_welcome"></a>Welcome</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to Maximize Your GPU ROI: Scaling LLM Inference llm-d with OpenShift AI!</p>
</div>
<div class="paragraph">
<p>This course is a practical Technical Level 2 (TL2) immersion designed for implementers, architects, consultants, and operators who are responsible for running Generative AI workloads in production.</p>
</div>
<div class="paragraph">
<p>We begin by tackling the core market challenge: the reality that ad-hoc AI experiments often fail to deliver ROI due due to skyrocketing OpEx and relying on monolithic, hard-to-scale architectures. This course provides the knowledge to transform those experiments into a repeatable, scalable, and economically viable Intelligence Factory on Red Hat OpenShift AI.</p>
</div>
<div class="paragraph">
<p>OpenShift AI abstracts the difficulting of mastering the llm-d Kubernetes-native framework, by providing the operational control plane and extends the high-performance vLLM inference engine.</p>
</div>
<div class="paragraph">
<p>Upon completion, you will understand when distributed inference with llm-d is the correct solution to deliver production-grade Generative AI workloads using OpenShift AI. That are faster, cheaper, and more manageable across a hybrid cloud environment, ensuring your platform is the right solution for managing multi-tenant clusters securely and efficiently.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Distributed Inference with llm-d</strong> is now generally available (GA) with OpenShift AI 3.0 release.</p>
</div>
<div class="paragraph">
<p>In this release, distributed inference with llm-d on OpenShift AI supports intelligent inference scheduling, multi-model serving, and disaggregated serving for improved GPU utilization on generative AI models.</p>
</div>
<div class="paragraph">
<p>For the latest updates, vist the <a href="https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/release_notes/new-features-and-enhancements_relnotes#enhancements" target="blank">Red Hat OpenShift AI 3.0 Release Notes</a></p>
</div>
</td>
</tr>
</table>
</div>
<details>
<summary class="title">OpenShift AI 3.0 Summarized Release Notes</summary>
<div class="content">
<div class="paragraph">
<p><strong>Supported Features in Red Hat OpenShift AI 3.0 and llm-d:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>User Interface (UI) for Deployment:</strong> OpenShift AI now includes a user interface for configuring and deploying Large Language Models (LLMs) on the <code>llm-d</code> Serving Runtime. This streamlined interface simplifies common deployment scenarios by offering essential configuration options with sensible defaults, while still allowing explicit selection of the <code>llm-d</code> runtime.</p>
</li>
<li>
<p><strong>Observability and Grafana Integration:</strong> Platform administrators can now connect observability components to Distributed Inference with <code>llm-d</code> deployments. This allows integration with self-hosted Grafana instances for monitoring inference workloads. Teams can collect and visualize Prometheus metrics from <code>llm-d</code> for performance analysis and custom dashboard creation.</p>
</li>
<li>
<p><strong>Model-as-a-Service (MaaS) Support:</strong> MaaS is currently supported only for models deployed using the Distributed Inference Server with <code>llm-d</code> runtime.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>The following capabilities are not fully supported in OpenShift AI 3.0:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Wide Expert-Parallelism multi-node: Developer Preview.</p>
</li>
<li>
<p>Wide Expert-Parallelism on Blackwell B200: Not available but can be provided as a Technology Preview.</p>
</li>
<li>
<p>Multi-node on GB200: Not supported.</p>
</li>
<li>
<p>Gateway discovery and association are not supported in the UI during model deployment in this release. Users must associate models with Gateways by applying the resource manifests directly through the API or CLI.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>llm-d Known Issues in OpenShift AI 3.0:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>While Distributed Inference Server with llm-d appears as an available option for deployment, it is not currently listed on the Serving Runtimes page under the Settings section.</p>
</li>
<li>
<p>Models deployed using llm-d initially display a <strong>Failed</strong> status on the Deployments page in the OpenShift AI dashboard, even if associated pod logs report no errors or failures; the status automatically updates to <strong>Started</strong> when the model is ready.</p>
</li>
</ul>
</div>
</div>
</details>
</div>
</div>
<div class="sect1">
<h2 id="_what_youll_learn"><a class="anchor" href="#_what_youll_learn"></a>What You’ll Learn</h2>
<div class="sectionbody">
<div class="paragraph">
<p>How distributed inference with <strong>llm-d</strong> on Red Hat OpenShift AI, delivers a superior value proposition across three critical areas:</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/llm-d_advantage.png" alt="llm-d OpenShift AI Advantage" width="500">
</div>
</div>
<div class="sect2">
<h3 id="_optimize_total_cost_of_ownership_tco"><a class="anchor" href="#_optimize_total_cost_of_ownership_tco"></a>Optimize Total Cost of Ownership (TCO):</h3>
<div class="ulist">
<ul>
<li>
<p>Eliminate "Paying for Idle GPUs" by implementing <strong>llm-d’s Intelligent Inference Scheduling</strong>, which replaces basic, AI-unaware Kubernetes scheduling to balance the cluster and maximize the usage of expensive hardware.</p>
</li>
<li>
<p>Additionally, discover how <strong>KV Cache Management</strong> offloads the model’s "memory" from expensive GPU VRAM to cheaper CPU RAM to maximize GPU density and allow running more models on the same hardware.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_guarantee_performance_service_level_objectives_slos"><a class="anchor" href="#_guarantee_performance_service_level_objectives_slos"></a>Guarantee Performance &amp; Service Level Objectives (SLOs):</h3>
<div class="ulist">
<ul>
<li>
<p>Move beyond the "Best Effort" performance of public APIs. Learn how <strong>llm-d’s Disaggregated Inference</strong> ensures interactive applications remain responsive by separating the compute-heavy "Prefill" phase from the memory-fast "Decode" phase .</p>
</li>
<li>
<p>LLM-D handles Mixture of Expert (MOE) models complexity by splitting these giant models across many nodes, managing the high-speed networking needed to make them work as one cohesive unit.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_ensure_total_security_control_sovereign_ai"><a class="anchor" href="#_ensure_total_security_control_sovereign_ai"></a>Ensure Total Security &amp; Control (Sovereign AI):</h3>
<div class="ulist">
<ul>
<li>
<p>Achieve <strong>"Sovereign AI"</strong> by deploying the entire high-performance inference stack inside your own OpenShift cluster, ensuring proprietary RAG documents, customer data, and sensitive prompts never leave your control.</p>
</li>
<li>
<p>This approach empowers you to <strong>Own Your Hybrid Cloud Strategy</strong>, deploying a secure, consistent, and performant AI inference platform wherever your data lives—on-premise, in a private cloud, or at the edge.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_walkthrough_experience"><a class="anchor" href="#_lab_walkthrough_experience"></a>Lab Walkthrough Experience</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This course provide an Interactive Lab Experience</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Using Red Hat Demo Platform Catalog item: OpenShift AI 3.0.</p>
</li>
<li>
<p>Adding depandant operators for Distributed Inference with llm-d.</p>
</li>
<li>
<p>Deploy a model using OpenShift AI&#8217;s vLLM serving runtime.</p>
</li>
<li>
<p>Use a Workbench to insure inference is working.</p>
</li>
<li>
<p>Deploy a model using OpenShift AI&#8217;s distributed inference component.</p>
</li>
</ul>
</div>
<hr>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This course was created with assistance from AI tools to accelerate content development and improve quality. However, all content is built, edited, tested, and reviewed by experienced human engineers to ensure technical accuracy and real-world applicability. We believe in transparency about our tools while maintaining human expertise and oversight at every step.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_additional_resources"><a class="anchor" href="#_additional_resources"></a>Additional Resources</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Appendix: Taxonomy to Know</p>
</li>
<li>
<p>Deep Dive on KV Cache Management</p>
</li>
<li>
<p>LeaderWorkerSet (LWS) Operator</p>
</li>
<li>
<p>Step by Step Hands on Lab</p>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="next"><a href="chapter3/redhatai.html">AI Factory on Red Hat AI</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
