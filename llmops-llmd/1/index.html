<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Maximize Your GPU ROI: Scaling LLM Inference with llm-d :: Maximize Your GPU ROI, Scaling LLM Inference llm-d</title>
    <link rel="next" href="chapter1/vllmllmd.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../..">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-llmd" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/vllmllmd.html">The Distributed Inference Stack: vLLM &amp; llm-d</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/llmdarch.html">System Mechanics: Building the Foundation &amp; Request Lifecycle</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/simulator.html">Mission Simulator: Distributed Inference Operations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/labguide.html">Lab Mission: Deploying Distributed Inference</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/summary.html">Mission Debrief: Maximizing ROI &amp; Next Steps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/well-lit-paths.html">Mission Data: Reference &amp; Glossary</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/trbleshoot.html">Mission Recovery: Troubleshooting &amp; FAQs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/kvcache.html">Deep Dive: The Physics of Latency (KV Cache)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="appendix/lws.html">LeaderWorkerSet (LWS) Operator</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Maximize Your GPU ROI, Scaling LLM Inference llm-d</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link is-current"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Maximize Your GPU ROI, Scaling LLM Inference llm-d</a></li>
    <li><a href="index.html">Maximize Your GPU ROI: Scaling LLM Inference with llm-d</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Maximize Your GPU ROI: Scaling LLM Inference with llm-d</h1>
<div class="sect1">
<h2 id="_red_hat_openshift_ai"><a class="anchor" href="#_red_hat_openshift_ai"></a>Red Hat OpenShift AI</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>Role:</strong> Platform Architect &amp; Engineer
<strong>Strategic Imperative:</strong> Transform Generative AI from a high-cost experiment into a scalable, economically viable enterprise service.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_the_executive_mandate_solving_the_ai_tax"><a class="anchor" href="#_the_executive_mandate_solving_the_ai_tax"></a>The Executive Mandate: Solving the "AI Tax"</h2>
<div class="sectionbody">
<div class="paragraph">
<p>"We cannot afford to let expensive infrastructure sit idle while users wait for results."</p>
</div>
<div class="paragraph">
<p>As organizations move Generative AI from pilot to production, they encounter a "Scale Wall"—a tripartite challenge that threatens the return on investment (ROI) of AI initiatives.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Skyrocketing Operational Costs:</strong> Paying for GPU capacity that sits significantly idle due to inefficient, monolithic scheduling.</p>
</li>
<li>
<p><strong>Unpredictable Performance:</strong> "Time-To-First-Token" (TTFT) spikes during high concurrency, frustrating users and missing Service Level Objectives (SLOs).</p>
</li>
<li>
<p><strong>Governance &amp; Sovereignty Risks:</strong> "Shadow AI" adoption creating data leaks, with sensitive proprietary data leaving the secure perimeter.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This course is your blueprint for solving these challenges. You will learn to deploy <strong>Distributed Inference with <code>llm-d</code></strong> on Red Hat OpenShift AI 3.0, moving from simple model serving to an <strong>intelligent routing inference architecture</strong>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_solution_intelligent_disaggregation"><a class="anchor" href="#_the_solution_intelligent_disaggregation"></a>The Solution: Intelligent Disaggregation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this Technical Level 2 (TL2) experience, we replace standard inference methods with <code>llm-d</code>, a Kubernetes-native framework designed to industrialize model serving.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/llm-d_advantage.png" alt="The llm-d Architectural Advantage" width="700">
</div>
<div class="title">Figure 1. The Strategic Advantage of llm-d</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>Business Value</strong></th>
<th class="tableblock halign-left valign-top"><strong>Technical Capability</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Maximize Infrastructure ROI</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Intelligent, Cache-Aware Routing</strong><br>
Stop over-provisioning. Unlike standard round-robin scheduling, <code>llm-d</code> routes requests to the specific node that already holds the user&#8217;s conversation context (KV Cache). This drastically reduces re-computation and allows you to pack more active users onto the same hardware.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Guarantee Performance (SLOs)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Prefill/Decode Disaggregation</strong><br>
Eliminate bottlenecks by splitting the compute-heavy "Prefill" (processing the prompt) from the latency-sensitive "Decode" (generating the response) into separate microservices. This ensures interactive applications remain responsive even under heavy load.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Ensure Sovereign Security</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Private Cloud Control</strong><br>
Deploy the entire high-performance stack inside your OpenShift cluster. Proprietary RAG documents, customer data, and sensitive prompts never leave your controlled environment, ensuring compliance with data sovereignty mandates.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_operational_intelligence_openshift_ai_3_0"><a class="anchor" href="#_operational_intelligence_openshift_ai_3_0"></a>Operational Intelligence: OpenShift AI 3.0</h2>
<div class="sectionbody">
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Read Before Deployment</div>
<div class="paragraph">
<p><strong>Distributed Inference with <code>llm-d</code> is Generally Available (GA) in OpenShift AI 3.0.</strong><br>
This release introduces significant capabilities for production-grade AI, but requires specific architectural awareness.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_1_whats_new_supported"><a class="anchor" href="#_1_whats_new_supported"></a>1. What&#8217;s New &amp; Supported</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Streamlined UI Deployment:</strong> Configure <code>llm-d</code> directly in the Dashboard with sensible defaults for rapid provisioning.</p>
</li>
<li>
<p><strong>Enterprise Observability:</strong> Full integration with Prometheus and user-managed Grafana to monitor critical "Golden Signals" like Token Generation Speed (TPOT) and Cache Hit Rates.</p>
</li>
<li>
<p><strong>Model-as-a-Service (MaaS):</strong> Currently supported explicitly for models deployed via the <code>llm-d</code> runtime.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_2_implementation_caveats"><a class="anchor" href="#_2_implementation_caveats"></a>2. Implementation Caveats</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Gateway Association:</strong> You cannot associate a Gateway directly in the UI during creation. You <strong>must</strong> apply the <code>HTTPRoute</code> resource via CLI/API after deployment to expose the service.</p>
</li>
<li>
<p><strong>Status Indicators:</strong> Models may initially show a <strong>Failed</strong> status in the dashboard while initializing. This is often a UI synchronization delay; check the pod logs. The status will automatically flip to <strong>Started</strong> when ready.</p>
</li>
<li>
<p><strong>Hardware Topologies:</strong> Multi-node support on GB200 is not currently supported. Wide Expert-Parallelism (multi-node sharding) is currently in Developer Preview.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_your_mission_checklist_the_lab"><a class="anchor" href="#_your_mission_checklist_the_lab"></a>Your Mission Checklist (The Lab)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will move beyond theory to build a functioning, governed inference stack. This interactive lab covers the full lifecycle[cite: 973]:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Foundation:</strong> Install the critical dependent Operators, including Red Hat Connectivity Link for security and the LeaderWorkerSet (LWS) Operator for orchestration.</p>
</li>
<li>
<p><strong>Baseline:</strong> Deploy a standard vLLM runtime to establish a performance benchmark.</p>
</li>
<li>
<p><strong>Evolution:</strong> Deploy the <strong>Distributed <code>llm-d</code> Component</strong> via the new OpenShift AI 3.0 interface and CLI to enable intelligent routing.</p>
</li>
<li>
<p><strong>Verification:</strong> Use a Workbench to run inference tests, validating latency reduction and routing logic.</p>
</li>
</ol>
</div>
<hr>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">A Note on Methodology</div>
<div class="paragraph">
<p>This content is engineered to reflect real-world production patterns. Every architectural decision—from security contexts to resource limits—has been validated by Red Hat engineers to ensure reliability.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mission_support_resources"><a class="anchor" href="#_mission_support_resources"></a>Mission Support Resources</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="appendix/index.html" class="xref page"><strong>Taxonomy:</strong> The Terminology of Distributed Systems</a></p>
</li>
<li>
<p><a href="appendix/kvcache.html" class="xref page"><strong>Deep Dive:</strong> Understanding KV Cache Management</a></p>
</li>
<li>
<p><a href="appendix/lws.html" class="xref page"><strong>Reference:</strong> The LeaderWorkerSet (LWS) Operator</a></p>
</li>
<li>
<p><a href="appendix/trbleshoot.html" class="xref page"><strong>Troubleshooting:</strong> Step-by-Step Lab Guide</a></p>
</li>
</ul>
</div>
<hr>
<div class="paragraph">
<p><strong>Content Architect:</strong><br>
Karlos Knox,<br>
Product Portfolio Marketing &amp; Learning</p>
</div>
<div class="paragraph">
<p><strong>Technical Validation &amp; Lab Guides:</strong></p>
</div>
<div class="paragraph">
<p>Fernando Lozano, Stephen Buck, Naina Singh, and Philip Hays.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="next"><a href="chapter1/vllmllmd.html">The Distributed Inference Stack: vLLM &amp; llm-d</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
