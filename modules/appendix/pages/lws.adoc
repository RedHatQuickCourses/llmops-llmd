= LeaderWorkerSet (LWS) Operator

[preface]
== Overview

The Leader Worker Set (LWS) Operator from the OpenShift Operator Hub is a prerequisite for OpenShift AI's advanced distributed inference capabilities.

This reference page explains what the LWS Operator is and why it is a critical piece of infrastructure for running modern AI/ML workloads.

== 1. The Core Problem: Scaling Distributed AI Workloads

Using Large Language Models (LLMs) for inference often requires massive compute resources. A single model can be so large that it must be "sharded" (split) across multiple GPUs on multiple different nodes.

This creates a significant deployment challenge for standard Kubernetes:

 * How do you manage a "group" of pods that must all work together as a single application?

 * How do you scale them up or down together as a single unit?

 * How do you handle a failure? If one pod in the group fails, the entire sharded model is unusable, so you need to restart the entire group, not just the one failed pod.

Standard Kubernetes objects like Deployments or StatefulSets are not designed to manage these tightly-coupled "groups" of pods as a single, atomic unit.

== 2. What is the LeaderWorkerSet (LWS)?

The LeaderWorkerSet is a custom Kubernetes API (implemented on OpenShift by the LWS Operator) designed specifically to solve this problem.

It gives you the ability to deploy and manage a group of pods as a single, coordinated, and atomic unit, sometimes called a "super pod."

=== 2.1. LWS Architecture

When you create a LeaderWorkerSet resource, you define a template for a "leader" pod and a "worker" pod, and specify how many workers are in each group (the size).

The LWS Operator then creates:

 * A Leader Pod: This is the "leader" of the group (index 0). It is often responsible for coordination or, as you'll see later, running the main server process.

 * A Worker StatefulSet: This StatefulSet is owned by the leader pod and manages all the "worker" pods in the group.

The entire group (1 leader + N workers) is treated as a single entity for its entire lifecycle.

---

=== 2.2. Key Features

LWS provides critical features for distributed AI workloads:

 * Group as a Unit: All pods in the group are created in parallel and share an identical lifecycle.

 * Group-Level Rollouts: When you update the LWS, it performs a rolling update at the group level (e.g., group 0 is terminated and replaced with a new group 0), ensuring the application's consistency.

 * All-or-Nothing Failure Handling: You can configure LWS to restart the entire group if even one pod in the group fails. This is essential for sharded models where the failure of one shard (pod) makes the entire group non-functional.

Topology-Aware Placement: LWS can ensure that all pods within the same group are co-located in the same topology (e.g., the same node, rack, or availability zone). This is critical for high-speed, low-latency networking (like InfiniBand or NVLink) required by distributed inference.

---

== 3. Reference Example: How LWS is Used with vLLM

While our lab uses the "Well-Lit Paths" provided by llm-d to manage deployments, it's useful to see how LWS is used by the open-source community to manage distributed inference engines like vLLM.

In this common pattern, LWS is used to deploy a single replica of a vLLM model that is sharded for tensor parallelism across multiple pods.

A LeaderWorkerSet is defined with a size of 8 (for 8 GPUs).

The Leader Pod (Pod 0): This pod is configured to act as the Ray "head node" and runs the main vLLM server, which receives the API requests.

The Worker Pods (Pods 1-7): These pods act as the Ray "worker nodes" and hold the other shards of the model for tensor-parallel execution.

When a request comes in, the leader pod coordinates with all 7 worker pods to perform the inference calculation as a single, distributed unit. If any of these 8 pods fail, LWS tears down the entire group and recreates it, ensuring the model replica is always in a consistent state.

---

== 4. Summary: Why We Install It

You are installing the Leader Worker Set Operator because it provides a foundational Kubernetes-native capability that OpenShift AI builds upon. It is the "engine" that enables the robust deployment, scaling, and lifecycle management of complex, multi-pod AI applications.