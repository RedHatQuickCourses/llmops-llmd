= Deep Dive on KV Cache Management

[TIP.tada]
.The KV Cache "Litmus Test"

The KV Cache is the #1 bottleneck for LLM performance. If you nod your head to these points, this deep dive is for you.

 - [x] ðŸ“ˆ You run high-concurrency services with many simultaneous users.

 - [x] ðŸ—£ï¸ Your application is a chatbot or RAG system that requires long conversation histories (context).

 - [x] ðŸ¢ You are suffering from high Time-To-First-Token (TTFT) as your request queues get backed up.

 - [x] ðŸ’¸ Your expensive GPU VRAM is constantly full, limiting the total number of users you can serve.

 - [x] ðŸš€ You want to achieve the absolute lowest latency and highest throughput possible from your hardware.

'''


[#deep-dive]
== Deep Dive: Understanding the KV Cache

Now that you understand why the cache is so important, let's explore what it is and the strategies to manage it.

=== What is the KV Cache?

The KV Cache is the model's "short-term memory."

When you send a prompt (like "What is the capital of France?"), the model performs two phases:

. *Prefill Phase:* It processes your prompt all at once. As it does, it generates intermediate data (the Keys and Values, or "KV") that represent the meaning and context of your prompt. This data is the KV Cache. This phase is compute-heavy and slow.
. *Decode Phase:* To generate an answer ("The", "capital", "is", "Paris"), the model looks at the KV Cache and generates one new token at a time. This phase is memory-bandwidth-heavy but very fast.

The KV Cache lives in the most expensive, limited resource you have: GPU VRAM.

---

=== Why is the Cache a Performance Bottleneck?

The KV Cache is the root of two major performance problems:

[cols="1a,1a"]
|===
|Problem 1: VRAM Exhaustion |Problem 2: The "Prefill" Bottleneck

|The KV Cache is huge. A single user with a long conversation history (like a RAG chatbot) can consume gigabytes of VRAM. This limits how many users can be served at the same time on one GPU.
|The "Prefill" step (which creates the cache) is slow and compute-intensive. In a traditional system, a new, long Prefill request will get in line and block all the fast Decode requests waiting behind it. This causes a "head-of-line blocking" problem and makes your whole service feel laggy.
|===

== OpenShift AI's distributed inference provides two main strategies to defeat both of these problems.



=== Caching Strategy 1: Independent Caching (VRAM to RAM Offload)

This is a "North-South" caching strategy, meaning the cache data moves within a single pod.

What it is: The vLLM engine inside the pod intelligently offloads "cold" (infrequently used) KV Caches from expensive GPU VRAM to more abundant, cheaper CPU RAM.

How it works: When a new user request comes in, its "cold" cache is quickly swapped from CPU RAM back into VRAM for the fast Decode phase.

When to use it: This is the default and is perfect for most workloads. It's an automatic way to fit more users onto a single GPU, maximizing your TCO.


=== Caching Strategy 2: Shared Caching (Distributed Cache Pool)

This is an "East-West" caching strategy, meaning the cache data moves between pods.

What it is: This strategy is used when Prefill and Decode are fully disaggregated (running on different pods). A shared, external cache pool (like Redis) is used as the "single source of truth" for the KV Caches.

How it works:
. A user sends a new prompt.
. The Gateway routes it to a Prefill pod.
. The Prefill pod does the heavy compute and writes the resulting KV Cache to the Shared Redis Cache.
. The Gateway then routes all future Decode requests for that user to any available Decode pod, which reads the cache from Redis.

When to use it: This is an advanced strategy for extreme-scale, high-concurrency workloads. It's more complex (requires a Redis cluster and high-speed networking) but offers the ultimate in flexibility, as you can scale your Prefill compute and Decode memory resources completely independently.


== Summary: When to Use Each Strategy

[cols="1a,2a,2a"]
|===
|Strategy |What It Solves |When to Use It (Implementation)

|Independent Caching
(VRAM -> RAM Offload)
|Problem: VRAM Exhaustion
|Default / Recommended: This is the standard, high-performance mode. It maximizes the user capacity of your individual GPU nodes.

|Shared Caching
(Distributed Pool)
|Problem: "Head-of-Line" Blocking
|Advanced / High-Scale: Use this when you are running a massive, multi-tenant service and need to independently scale your Prefill compute (for long prompts) from your Decode capacity (for high user count).
|===