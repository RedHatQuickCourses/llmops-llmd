= Mission Data: Reference & Glossary
:toc: macro
:toc-title: Technical Reference

// Header Image placeholder
image::https://www.redhat.com/cms/managed-files/styles/wysiwyg_full_width/s3/Standard%20Logo.png?itok=media_token[Red Hat OpenShift AI, 200, align="left"]

[ABSTRACT]
**Role:** All Personas
**Objective:** Definitions of core taxonomy and low-level infrastructure components.

---

== Taxonomy: Speaking the Language

[cols="1,3"]
|===
| **Term** | **Definition**

| **vLLM**
| The high-performance inference engine. It runs the model, but `llm-d` scales it.

| **Inference Gateway (EPP)**
| The "Brain." The smart router that intercepts requests and decides which pod handles them based on cache affinity.

| **Prefill**
| Phase 1 of Inference. Processing the user's prompt. Compute-heavy and slow.

| **Decode**
| Phase 2 of Inference. Generating the response. Memory-bandwidth heavy and fast.

| **KV Cache**
| The "Short-Term Memory" of the model. Reusing this via smart routing is the primary way to cut costs.

| **TTFT**
| **Time-To-First-Token.** Responsiveness metric (how long until the first word appears?).
|===

== Infrastructure Deep Dive: LeaderWorkerSet (LWS)

The **LeaderWorkerSet (LWS)** Operator is a prerequisite for advanced distributed inference. It solves the "Gang Scheduling" problem in Kubernetes.

* **The Problem:** Large models are sharded across multiple GPUs (e.g., 8 GPUs). If one pod fails, the whole model breaks. Standard Kubernetes Deployments don't know this.
* **The LWS Solution:** It manages a group of pods as a single atomic unit ("Super Pod").
    * **Group Lifecycle:** If one pod fails, LWS restarts the entire group to ensure consistency.
    * **Topology Awareness:** Ensures all pods in the group are co-located (same rack/node) for high-speed interconnects (NVLink/InfiniBand).