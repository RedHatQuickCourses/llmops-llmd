= vLLM to AI Factory Scaling with LLM-D


=== Increase Performance and Reduce Cost with Distributed Inference

---
==== This course is your guide to understand the specific, pre-packaged deployment patterns provided by *llm-d* that solve real-world scaling challenges, from immediate wins to advanced optimization.
---

[IMPORTANT]
.Prerequisite Knowledge

This course assumes you are already familiar with the basics of Kubernetes and vLLM Inference. We will be building on those core concepts.


== The Core Challenge: APIs vs. On-Premise AI

Generative AI is transforming industries, but as you move from experiment to production, you face a critical business decision:

[quote]
"Why should we build, manage, and pay for our own GPU infrastructure when cheap, pay-as-you-go public API tokens exist?"

A public API offers convenience, but at the cost of control. It's a "best-effort" service with unpredictable performance, runaway costs for high-volume tasks, and a critical security blindspot: you must send your most sensitive corporate data to a third party.

This course will show you how to build a true "AI Factory" on OpenShift AI, using the power of distributed inference with llm-d to achieve what public APIs cannot.

== The Value of an AI Factory

How OpenShift AI and Distributed Inference (powered by llm-d) give you what public APIs cannot:

=== Optimize Total Cost of Ownership (TCO)

 * *Stop "Paying for Idle GPUs":* Learn how llm-d's Smart Scheduling (Path 1) replaces basic, "AI-unaware" Kubernetes scheduling. You'll see how to eliminate hotspots, balance your cluster, and finally use 100% of the expensive hardware you've already paid for.

 * *Maximize GPU Density:* Discover how KV Cache Management (Path 4) offloads the model's "memory" from expensive GPU VRAM to cheaper CPU RAM, allowing you to run more models on the same hardware.

=== Guarantee Performance & SLOs

 * *Move from "Best Effort" to "Guaranteed":* Public APIs offer no performance guarantees. Learn how llm-d's Disaggregated Inference (Path 2) separates compute-heavy "Prefill" from memory-fast "Decode" to ensure your interactive applications are always responsive.

 * *Master the "Well-Lit Paths":* Understand the 4 pre-packaged, Helm-automated deployment patterns that provide optimized, step-by-step solutions for scaling any model, from the simplest to the most complex.

=== Ensure Total Security & Control

 * *Achieve "Sovereign AI"*: The most critical value-add. Learn how this entire, high-performance stack runs inside your own OpenShift cluster. Your proprietary RAG documents, customer data, and sensitive prompts never leave your control.

 * *Own Your Hybrid Cloud Strategy:* Deploy a secure, consistent, and performant AI inference platform wherever your data livesâ€”on-premise, in a private cloud, or at the edge.

