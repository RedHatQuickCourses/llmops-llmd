= Maximize Your GPU ROI: Scaling LLM Inference with llm-d
:toc: macro
:toc-title: Strategic Overview

// Header Image placeholder
image::llmd-banner.png[Red Hat OpenShift AI, 200, align="left"]

[ABSTRACT]
**Role:** Platform Architect & Engineer +
**Strategic Imperative:** Transform Generative AI from a high-cost experiment into a scalable, economically viable enterprise service.

---

== The Executive Mandate: Solving the "AI Tax"

"We cannot afford to let expensive infrastructure sit idle while users wait for results."

As organizations move Generative AI from pilot to production, they encounter a "Scale Wall"—a triple challenge that threatens the return on investment (ROI) of AI initiatives.

 .  **Skyrocketing Operational Costs:** Paying for GPU capacity that sits significantly idle due to inefficient, single pod scheduling.
 .  **Unpredictable Performance:** "Time-To-First-Token" (TTFT) spikes during high concurrency, frustrating users and missing Service Level Objectives (SLOs).
 .  **Governance & Sovereignty Risks:** "Shadow AI" adoption creating data leaks, with sensitive proprietary data leaving the secure perimeter.

This course is your blueprint for solving these challenges. You will learn to deploy **Distributed Inference with `llm-d`** on Red Hat OpenShift AI 3.0, moving from simple model serving to an **intelligent routing inference architecture**.



== The Solution: Intelligent Disaggregation

In this Technical Level 2 (TL2) experience, we replace standard inference methods with `llm-d`, a Kubernetes-native framework designed to industrialize model serving.

.The Strategic Advantage of llm-d
image::llm-d_advantagev2.png[The llm-d Architectural Advantage, 700, align="center"]

[cols="1,3"]
|===
| **Business Value** | **Technical Capability**

| **Maximize Infrastructure ROI**
| **Intelligent, Cache-Aware Routing** +
Stop over-provisioning. Unlike standard round-robin scheduling, `llm-d` routes requests to the specific node that already holds the user's conversation context (KV Cache). This drastically reduces re-computation and allows you to pack more active users onto the same hardware.

| **Guarantee Performance (SLOs)**
| **Prefill/Decode Disaggregation** +
Eliminate bottlenecks by splitting the compute-heavy "Prefill" (processing the prompt) from the latency-sensitive "Decode" (generating the response) into separate microservices. This ensures interactive applications remain responsive even under heavy load.

| **Ensure Sovereign Security**
| **Private Cloud Control** +
Deploy the entire high-performance stack inside your OpenShift cluster. Proprietary RAG documents, customer data, and sensitive prompts never leave your controlled environment, ensuring compliance with data sovereignty mandates.
|===

== Operational Intelligence: OpenShift AI 3.0

[IMPORTANT]
.Read Before Deployment
====
**Distributed Inference with `llm-d` is Generally Available (GA) in OpenShift AI 3.0.** +
This release introduces significant capabilities for production-grade AI, but requires specific architectural awareness.
====

=== 1. What's New & Supported
* **Streamlined UI Deployment:** Configure `llm-d` directly in the Dashboard with sensible defaults for rapid provisioning.
* **Enterprise Observability:** Full integration with Prometheus and user-managed Grafana to monitor critical "Golden Signals" like Token Generation Speed (TPOT) and Cache Hit Rates.
* **Model-as-a-Service (MaaS):** Currently supported explicitly for models deployed via the `llm-d` runtime.

=== 2. Implementation Caveats
* **Status Indicators:** Models may initially show a *Failed* status in the dashboard while initializing. This is often a UI synchronization delay; check the pod logs. The status will automatically flip to *Started* when ready.
* **Hardware Topologies:** Multi-node support on GB200 is not currently supported. Wide Expert-Parallelism (multi-node sharding) is currently in Developer Preview.

== Your Mission Checklist (The Lab)

We will move beyond theory to build a functioning, governed inference stack. This interactive lab covers the full lifecycle:

1.  **Foundation:** Install the critical dependent Operators, including Red Hat Connectivity Link for security and the LeaderWorkerSet (LWS) Operator for MOE orchestration.
2.  **Baseline:** Deploy a standard vLLM runtime to establish a performance benchmark. (simulation section only)
3.  **Evolution:** Deploy the **Distributed `llm-d` Component** via the new OpenShift AI 3.0 interface and CLI to enable intelligent routing.
4.  **Verification:** Use a Workbench to run inference tests, validating latency reduction and routing logic.

---

[TIP]
.A Note on Methodology
====
Engineered for Reality: This content reflects verified production patterns and utilizes AI-assisted assets. Every technical configuration—from resource limits to security—has been vetted by human experts. Note: Due to the fast pace of Distributed AI, specific methods may evolve quickly.
====

== Support Resources

* xref:appendix:well-lit-paths.adoc[The "Well-Lit Paths" of Deployment, window="blank"] 
* xref:appendix:trbleshoot.adoc[Troubleshooting & FAQs, window="blank"] 
* xref:appendix:kvcache.adoc[Deep Dive: KV Cache), window="blank"]
* xref:appendix:reference.adoc[Reference & Glossary, window="blank"]
* xref:https://youtu.be/CNKGgOphAPM?si=svtIu-tscvtN1raW[LLM‑D Explained: Building Next‑Gen AI with LLMs, RAG & Kubernetes
, window='blank']
'''

*Content Architect:* +
Karlos Knox, +
Product Portfolio Marketing & Learning 

*Special Thanks to these individuals for technical suggestions, validation, and example lab guides:* 

Fernando Lozano, Stephen Buck, Naina Singh, and Philip Hays.