= Maximize Your GPU ROI: Scaling LLM Inference llm-d


== Welcome

Welcome to Maximize Your GPU ROI: Scaling LLM Inference llm-d with OpenShift AI!

This course is a practical Technical Level 2 (TL2) immersion designed for implementers, architects, consultants, and operators who are responsible for running Generative AI workloads in production. 

We begin by tackling the core market challenge: the reality that ad-hoc AI experiments often fail to deliver ROI due due to skyrocketing OpEx and relying on monolithic, hard-to-scale architectures. This course provides the knowledge to transform those experiments into a repeatable, scalable, and economically viable Intelligence Factory on Red Hat OpenShift AI.

OpenShift AI abstracts the difficulting of mastering the llm-d Kubernetes-native framework, by providing the operational control plane and extends the high-performance vLLM inference engine.

Upon completion, you will understand when distributed inference with llm-d is the correct solution to deliver production-grade Generative AI workloads using OpenShift AI. That are faster, cheaper, and more manageable across a hybrid cloud environment, ensuring your platform is the right solution for managing multi-tenant clusters securely and efficiently.


[NOTE]
====
*Distributed Inference with llm-d* is now generally available (GA) with OpenShift AI 3.0 release.  

In this release, distributed inference with llm-d on OpenShift AI supports intelligent inference scheduling, multi-model serving, and disaggregated serving for improved GPU utilization on generative AI models.

For the latest updates, vist the https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/release_notes/new-features-and-enhancements_relnotes#enhancements[Red Hat OpenShift AI 3.0 Release Notes, new features, section 3.2 enhancements., window=blank]
====

[%collapsible]
.OpenShift AI 3.0 Summarized Release Notes
====

*Supported Features in Red Hat OpenShift AI 3.0 and llm-d:*

 *   **User Interface (UI) for Deployment:** OpenShift AI now includes a user interface for configuring and deploying Large Language Models (LLMs) on the `llm-d` Serving Runtime. This streamlined interface simplifies common deployment scenarios by offering essential configuration options with sensible defaults, while still allowing explicit selection of the `llm-d` runtime.
 *   **Observability and Grafana Integration:** Platform administrators can now connect observability components to Distributed Inference with `llm-d` deployments. This allows integration with self-hosted Grafana instances for monitoring inference workloads. Teams can collect and visualize Prometheus metrics from `llm-d` for performance analysis and custom dashboard creation.
 *   **Model-as-a-Service (MaaS) Support:** MaaS is currently supported only for models deployed using the Distributed Inference Server with `llm-d` runtime.

*The following capabilities are not fully supported in OpenShift AI 3.0:*

 * Wide Expert-Parallelism multi-node: Developer Preview.
 * Wide Expert-Parallelism on Blackwell B200: Not available but can be provided as a Technology Preview.
 * Multi-node on GB200: Not supported.
 * Gateway discovery and association are not supported in the UI during model deployment in this release. Users must associate models with Gateways by applying the resource manifests directly through the API or CLI.


*llm-d Known Issues in OpenShift AI 3.0:*

 *   While Distributed Inference Server with llm-d appears as an available option for deployment, it is not currently listed on the Serving Runtimes page under the Settings section.
 *   Models deployed using llm-d initially display a *Failed* status on the Deployments page in the OpenShift AI dashboard, even if associated pod logs report no errors or failures; the status automatically updates to *Started* when the model is ready.

====


== What You’ll Learn


How distributed inference with **llm-d** on Red Hat OpenShift AI, delivers a superior value proposition across three critical areas:

image::llm-d_advantage.png[llm-d OpenShift AI Advantage, 700, align="center"]

[%collapsible]
.The llm-d Advantage Text
====

**Optimize Total Cost of Ownership (TCO):**

 * Eliminate "Paying for Idle GPUs" by implementing **llm-d’s Intelligent Inference Scheduling**, which replaces basic, AI-unaware Kubernetes scheduling to balance the cluster and maximize the usage of expensive hardware. 
 * Additionally, discover how **KV Cache Management** offloads the model’s "memory" from expensive GPU VRAM to cheaper CPU RAM to maximize GPU density and allow running more models on the same hardware.

**Guarantee Performance & Service Level Objectives (SLOs):**

 * Move beyond the "Best Effort" performance of public APIs. Learn how **llm-d’s Disaggregated Inference** ensures interactive applications remain responsive by separating the compute-heavy "Prefill" phase from the memory-fast "Decode" phase . 
 * LLM-D handles Mixture of Expert (MOE) models complexity by splitting these giant models across many nodes, managing the high-speed networking needed to make them work as one cohesive unit.

**Ensure Total Security & Control (Sovereign AI):**

 * Achieve **"Sovereign AI"** by deploying the entire high-performance inference stack inside your own OpenShift cluster, ensuring proprietary RAG documents, customer data, and sensitive prompts never leave your control. 

 * This approach empowers you to **Own Your Hybrid Cloud Strategy**, deploying a secure, consistent, and performant AI inference platform wherever your data lives—on-premise, in a private cloud, or at the edge.

====

==  Lab Walkthrough Experience

This course provide an Interactive Lab Experience 

*   Using Red Hat Demo Platform Catalog item: OpenShift AI 3.0.
*   Adding depandant operators for Distributed Inference with llm-d.
*   Deploy a model using OpenShift AI's vLLM serving runtime.
*   Use a Workbench to insure inference is working.
*   Deploy a model using OpenShift AI's distributed inference component.

---

NOTE: This course was created with assistance from AI tools to accelerate content development and improve quality. However, all content is built, edited, tested, and reviewed by experienced human engineers to ensure technical accuracy and real-world applicability. We believe in transparency about our tools while maintaining human expertise and oversight at every step.

== Additional Appendix Resources
*  xref:appendix:index.adoc[Taxonomy to Know]
*  xref:appendix:kvcache.adoc[Deep Dive on KV Cache Management]
*  xref:appendix:lws.adoc[LeaderWorkerSet (LWS) Operator]
*  xref:appendix:trbleshoot.adoc[Step by Step Hands on Lab]

