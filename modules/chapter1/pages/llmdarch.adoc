= System Mechanics: Building the Foundation & Request Lifecycle
:toc: macro
:toc-title: Engineering Deep Dive

// Header Image placeholder
image::llmd-banner.png[Red Hat OpenShift AI, 200, align="left"]

[ABSTRACT]
**Role:** Platform Engineer +
**Technical Objective:** Master the component prerequisites and visualize the six-step "Intelligent Routing" lifecycle that occurs for every inference request.

---

== 1. The Foundation: Prerequisites
Before deploying the `llm-d` stack, specific infrastructure components must be present. Failure to meet these requirements is the primary cause of implementation issues.

[cols="1,2,2"]
|===
| **Component** | **Requirement** | **The "Why"**

| **Kubernetes**
| OpenShift 4.19+
| A modern stack relying on up-to-date features and the Gateway API.

| **Gateway API**
| Compliant implementation (e.g., OpenShift Service Mesh or kGateway)
| Acts as the "front door" for all traffic and the required integration point for the smart scheduler.

| **Hardware**
| Data Center GPUs (H100, A100, L40S)
| The vLLM engine is optimized specifically for these architectures, not consumer cards.

| **Operators**
| NVIDIA GPU Operator & Node Feature Discovery (NFD)
| Manages drivers and advertises GPU features to the Kubernetes scheduler.
|===

[WARNING]
.Critical: Network Fabric Requirements
====
**Do not underestimate the network.** +
High-performance "East-West" cache sharing requires a low-latency fabric (InfiniBand or RoCE). A standard 10GbE TCP/IP network will be a bottleneck and is **not supported** for advanced features like Mixture-of-Experts (MoE).
====

== 2. The Core Components
The `llm-d` architecture orchestrates several specialized controllers and routing logic.



* **Gateway API + Inference Extension (Envoy):** The "Smart Router" living in the `openshift-ingress` namespace. It intercepts user `HTTPRoute` requests and queries the scheduler.
* **Inference Scheduler (EPP):** The "Brain." It maintains a real-time view of all vLLM pods and runs "Filter-and-Score" logic to select the best destination.
* **vLLM Pods (Workers):** The execution engines running in the user namespace. In advanced setups, these are split into **Prefill** (compute-optimized) and **Decode** (memory-optimized) pools.
* **KServe GenAI Controller:** The "Orchestrator" within the Red Hat OpenShift AI Operator. It watches for a `ModelService` CRD and builds the underlying Deployment, Service, and Route objects.

== 3. The Data Flow: The Request Lifecycle
Understanding this six-step flow is critical for troubleshooting connectivity and routing behavior. The Scheduler makes the decision, but the Gateway moves the data.



1.  **Request In:** A user sends an OpenAI-compatible request (e.g., `/v1/chat/completions`) to the defined `HTTPRoute`.
2.  **Gateway Intercepts:** The Gateway API (Envoy) in `openshift-ingress` receives the traffic.
3.  **The "Brain" is Called:** The Gateway forwards the **request metadata** to the Inference Scheduler (EPP), effectively asking: *"Where should this go?"*.
4.  **Scheduler Decides (Filter & Score):**
    * **Filter:** The scheduler removes unhealthy, overloaded, or incompatible pods from the list.
    * **Score:** It scores remaining pods based on queue length and **KV Cache Affinity** (checking if a pod already holds the conversation context).
5.  **Instruction Issued (Control Plane):**
    * **Cache Hit (Fast Path):** The Scheduler **instructs the Gateway** to route traffic to the specific Decode pod holding the cache.
    * **Cache Miss (Slow Path):** The Scheduler **instructs the Gateway** to route traffic to a Prefill pod first.
6.  **Gateway Execution (Data Plane):** The Gateway receives the instruction and forwards the actual data payload to the chosen pod. The pod generates the response and streams it back through the Gateway to the user.

== 4. Observability: Proving It Works
To demonstrate ROI, you must track the "Golden Signals" of inference performance.

[cols="1,1,2"]
|===
| **Business Goal** | **Prometheus Metric** | **Description**

| **Prove Responsiveness**
| `vllm_llmd_time_to_first_token_seconds` (TTFT)
| The primary measure of perceived speed. Keep this low and stable.

| **Prove Throughput**
| `vllm_llmd_time_per_output_token_seconds` (TPOT)
| The "generation speed" metric.

| **Prove Cost Savings (TCO)**
| `vllm_llmd_kv_cache_hit_rate`
| **Your #1 ROI Metric.** A 90% hit rate means you are skipping expensive Prefill compute 90% of the time.

| **Prove Efficiency**
| `vllm_llmd_gpu_utilization_seconds`
| Confirms expensive GPUs are active and not idling.
|===