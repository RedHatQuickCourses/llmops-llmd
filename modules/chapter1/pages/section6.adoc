= Lab Demo: llm-d Deployment Guided Arcade Experience

[NOTE.icon-cogs]
.Your Starting Point
This page provides a series of interactive (Arcade) walkthroughs to help you provision and configure your OpenShift and OpenShift AI environment. You will use the Red Hat Demo Platform (RHDP) to order your cluster, configure the necessary components, and deploy the a small Granite 4 using distributed inference with llm-d on a single GPU. 

== Follow these six steps in order to complete the lab.


=== 1. Requesting the Lab Environment

This first Arcade experience demonstrates how to request your lab environment from the Red Hat Demo Platform (RHDP) catalog.

Below is the direct link to the catalog item you need to order. The Arcade will guide you through the ordering process.

This environment automatically deploys a Llama-3.2-3b Generative AI model to enable LlamaStack features in OpenShift AI. Please note that this automatically consumes GPU resources on the 4x and 8x instance types.

If you do not intend to use LlamaStack, select smaller instance types to save on costs,  stopping or removing the Llama-3.2-3b model to free up the GPU.

Selecting the g6.12xlarge instance will provision a single node with four 24GB GPUs, enabling the deployment of large models and intra-GPU scenarios.

https://catalog.demo.redhat.com/catalog?item=babylon-catalog-prod/published.openshift-ai-v3.prod&utm_source=webapp&utm_medium=share-link[Red Hat OpenShift AI 3.0, window=blank]



++++

<iframe src="https://demo.arcade.software/w6HfuWHzkJ0rx9yXCzQk?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++



== 2. Initial Cluster Login & Validation

This interactive experience provides a brief tour of the new menu options in OpenShift AI 3.0. In addition, we review the now-mandatory Hardware Profiles section, where predefined instance sizes can be configured to manage compute resources.

Next, we deploy a Granite model from the AI Hub Catalog to validate that the model inference components are fully available for standard vLLM deployments.


++++

<iframe src="https://demo.arcade.software/GfqvGnWw1uQdlBhkLhy9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++



== 3. OpenShift Console Configuiration and Settings

This interactive experience begins with your first hands-on steps: logging into the OpenShift Console to validate that the necessary Operators and OCP versions are ready to support llm-d.

We will first review the installed Operators from the ecosystem menu. Following this, we will install the Leader Worker Set Operator, which is required for llm-d to manage a fleet of pods for distributed inference.

Next, we will verify the hardware to ensure sufficient capacity is available and confirm that the OpenShift version is at least 4.19.

Finally, we will double-check that all operators have installed successfully before proceeding to the next segment.


++++

<iframe src="https://demo.arcade.software/S1dHIjii1Vjkqi0phdSd?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++


== 4. Deploying an Model using llm-d via the OpenShift AI Console

This interactive experience begins in the OpenShift AI console and quickly proceeds to deploy a Granite model using the llm-d runtime.

We start by selecting Deploy a model from the project menu. Next, we utilize an existing data connection to provide the Granite AI model files.

Proceed by entering the model name and selecting the runtime. For the Hardware Profile, ensure you select the profile configured for GPU usage.

Finally, deselect the option to use token authentication. In this specific environment, keeping token authentication enabled may prevent the deployment from completing successfully.

Optionally, you can choose to make this model available in the Playground (also known as the OpenShift AI Studio).

++++

<iframe src="https://demo.arcade.software/WDbYI8agiNhgCIfYPLp9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++




== 5. View the Deployment Progress in OpenShift


In this interactive experience, we return to the OpenShift Console to monitor the deployment of our llm-d inference service.

First, we will verify the deployed Pods. We should expect to see two specific pods: the intelligent scheduler and the vLLM-powered inference Pod responsible for running the model.

Next, we will examine the Topology view, logs, and events to track the progress of our deployment.



++++

<iframe src="https://demo.arcade.software/z3WQEOr7Vd1QrGmxVRQf?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"  width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++



== 6. Validating the Deployment

In this final interactive experience, we execute a query to confirm that our Inference Server is successfully deployed, responding to requests, and performing as expected.

We will start by using a Workbench to launch a Jupyter Notebook environment. Next, we will clone a Git repository containing the notebook that provides the specific instructions for querying our model.

After running the initial cell, you must customize the Model Name and the Inference Endpoint. Be sure to set the API Key to a null value, as we disabled token authentication during the deployment phase.

Finally, we will send a couple of distinct queries to validate the model's functionality and response quality. 

++++

<iframe src="https://demo.arcade.software/cKkaV0pgSsSJnRM0wRsj?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="600px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++


