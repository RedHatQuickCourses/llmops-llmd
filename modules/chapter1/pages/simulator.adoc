= Mission Simulator: Distributed Inference Operations
:toc: macro
:toc-title: Flight Deck

// Header Image placeholder
image::llmd-banner.png[Red Hat OpenShift AI, 200, align="left"]

[ABSTRACT]
**Role:** All Personas (No Lab Required)
**Objective:** Execute a complete end-to-end deployment cycle—from infrastructure provisioning to validation—in a guided, risk-free simulation.

---

== The Interactive Experience

Welcome to the **Mission Simulator**. This module provides a high-speed, guided tour of the entire `llm-d` workflow on Red Hat OpenShift AI 3.0.

Use this simulator if:

 * You do not have access to a GPU-enabled OpenShift cluster today.
 * You need to validate the "ClickOps" workflow before building automation.
 * You want to see the "Big Picture" operations without waiting for provisioning times.

---

== Phase 1: Infrastructure Provisioning (The Foundation)

Before we deploy models, we must secure the factory floor. These simulations cover the extraction of raw compute and the verification of the underlying operators.

=== 1.1 Requesting the Environment
**Task:** Provision a Red Hat OpenShift AI cluster with the correct GPU hardware profiles from the catalog.
++++
<iframe src="https://demo.arcade.software/w6HfuWHzkJ0rx9yXCzQk?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

=== 1.2 Access & Hardware Verification
**Task:** Log in to the console and inspect the "Hardware Profiles" to understand how we carve up physical GPUs for inference.
++++
<iframe src="https://demo.arcade.software/GfqvGnWw1uQdlBhkLhy9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

=== 1.3 Operator Health Check
**Task:** Verify the "Leader Worker Set (LWS)" Operator is active. This is the secret sauce `llm-d` uses to manage pod fleets.
++++
<iframe src="https://demo.arcade.software/S1dHIjii1Vjkqi0phdSd?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

---

== Phase 2: The Build (Deployment)

Now that the foundation is solid, we execute the deployment. These simulations show the transition from "Config" to "Running Service."

=== 2.1 Deploying with llm-d
**Task:** Configure the `llm-d` Serving Runtime, attach the Granite model, and select the distributed hardware profile.
++++
<iframe src="https://demo.arcade.software/WDbYI8agiNhgCIfYPLp9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

=== 2.2 Observability & Topology
**Task:** Inspect the OpenShift Topology view to confirm the separation of the **Inference Scheduler** (Control Plane) and **vLLM Pods** (Engine).
++++
<iframe src="https://demo.arcade.software/z3WQEOr7Vd1QrGmxVRQf?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"  width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

---

== Phase 3: Mission Validation (The Test)

A deployment is only successful if it answers questions.

=== 3.1 Inference Validation
**Task:** Launch a Jupyter Notebook, connect to the new inference endpoint, and validate the response latency.
++++
<iframe src="https://demo.arcade.software/cKkaV0pgSsSJnRM0wRsj?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

```