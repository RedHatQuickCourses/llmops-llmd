= Simulator: Distributed Inference Operations
:toc: macro
:toc-title: Flight Deck

// Header Image placeholder
image::llmd-banner.png[Red Hat OpenShift AI, 200, align="left"]

[ABSTRACT]
**Role:** All Personas (No Lab Required) +
**Objective:** Execute a complete end-to-end deployment cycle—from infrastructure provisioning to validation—in a guided, risk-free simulation.

---

== The Interactive Experience

Welcome to the **Distributed Inference Simulator**. This module provides a high-speed, guided tour of the entire `llm-d` workflow on Red Hat OpenShift AI 3.0.

Use this simulator if:

 * You do not have access to a GPU-enabled OpenShift cluster today.
 * You need to validate the "ClickOps" workflow before building automation.
 * You want to see the "Big Picture" operations without waiting for provisioning times.

---


== Phase 1: Infrastructure Provisioning

Before we deploy models, we must secure the OpenShift AI 3.0 Environment. This simulations use Red Hat demo plaform to provision a RHOAI 3.0 platform.

=== 1.1 Access & Hardware Verification
**Task:** Log in to the RHOAI console, review the dashboard menus, inspect the settings for "Hardware Profiles" to understand how allocate physical GPUs for inference, then deploy a model from the RHOAI Model Catalog.
++++
<iframe src="https://demo.arcade.software/GfqvGnWw1uQdlBhkLhy9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

=== 1.2 Operator Health Check
**Task:** From the Openshift Dashboard install the "Leader Worker Set (LWS)" Operator. This is the secret sauce `llm-d` uses to manage pod fleets.  Next, explore the OpenShift console to validate, OCP version of 4.19 or greater, with 4.20 recommended.  Install the Terminal Operator to interact with our cluster via the command line.
++++
<iframe src="https://demo.arcade.software/S1dHIjii1Vjkqi0phdSd?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

---

== Phase 2: The Distributed Inference Deployment

Now that the foundation is solid, we execute the deployment. These simulations show the transition from "Config" to "Running Service."

=== 2.1 Deploying with llm-d
**Task:** Configure the `llm-d` Serving Runtime, attach the Granite model, and select the distributed hardware profile.
++++
<iframe src="https://demo.arcade.software/WDbYI8agiNhgCIfYPLp9?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

=== 2.2 Observability & Topology
**Task:** Inspect the OpenShift Topology view to confirm the separation of the **Inference Scheduler** (Control Plane) and **vLLM Pods** (Engine). Explore the pods state during the llm-d inference deployment via the OpenShift console.
++++
<iframe src="https://demo.arcade.software/z3WQEOr7Vd1QrGmxVRQf?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"  width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

---

== Phase 3: Validation (The Test)

A deployment is only successful if it answers questions.

=== 3.1 Inference Validation
**Task:** Launch a Jupyter Notebook, connect to the new inference endpoint, and validate the response and percieved latency.
++++
<iframe src="https://demo.arcade.software/cKkaV0pgSsSJnRM0wRsj?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true" width="100%" height="500px" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen allow="clipboard-write" muted> </iframe>
++++

```