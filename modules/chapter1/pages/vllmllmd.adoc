= The Distributed Inference Stack: vLLM & llm-d
:toc: macro
:toc-title: Architecture Blueprint

// Header Image placeholder
image::llmd-banner.png[Red Hat OpenShift AI, 200, align="left"]

[ABSTRACT]
**Role:** System Architect +
**Technical Objective:** Understand the "Stacked Architecture" that decouples orchestration (Control Plane) from execution (Inference Engine).

---

== Industrializing Generative AI



To move from "Experiment" to "Enterprise Service," we must stop treating model serving as a single, monolithic pod.  Industrial-grade inference requires two distinct layers of optimization:

1.  **A High-Performance Engine:** To manage raw speed and memory at the node level.
2.  **An Intelligent Control Plane:** To manage resource efficiency and routing across the entire cluster.
Red Hat OpenShift AI combines the industry-standard **vLLM** engine with the Kubernetes-native **llm-d** framework to create this unified, production-grade stack.

== The Stack Architecture
The following architectural model visualizes the functional separation of the inference stack.



=== Layer 1: The Control Plane (llm-d)**Role:** Orchestration & Routing
At the top of the stack is **llm-d**, a Kubernetes-native framework that transforms monolithic deployments into intelligent microservices. It acts as the "Traffic Cop" for your compute.

* **Intelligent Scheduling:** Unlike standard Kubernetes round-robin routing, `llm-d` utilizes live metadata to route requests to the "warmest" nodes (KV Cache Affinity).
* **Disaggregation:** It splits the compute-heavy "Prefill" phase from the latency-sensitive "Decode" phase into separate services, optimizing hardware utilization.
* **MoE Parallelism:** It manages the complex inter-node communication required for massive Mixture-of-Expert (MoE) models that exceed single-node capacity.

=== Layer 2: The Service Layer (vLLM) **Role:** High-Throughput Execution

The middle layer is powered by **vLLM**, the state-of-the-art open-source engine. In this stack, vLLM is the worker engine running inside the pods managed by `llm-d`.

* **PagedAttention:** Manages KV cache memory in fixed-size pages (similar to OS virtual memory), eliminating fragmentation and enabling large context windows.
* **Continuous Batching:** Processes multiple queries together dynamically to maximize GPU saturation.
* **Speculative Decoding:** Predicts future tokens in parallel to speed up the generation phase.

=== Layer 3: The Hardware Layer**Role:** Base Compute Resources 

At the foundation are the physical resources (e.g., NVIDIA H100, A100). The software stack ensures these expensive assets are never wasted:

* **vLLM** ensures the GPU is not idle during execution cycles. 
* **llm-d** ensures requests are routed to the right GPU at the right time.

[NOTE]
.The Logistics Analogy
====

Think of running an LLM at scale like managing a global logistics network:

* **llm-d** is the **Air Traffic Control Tower**, optimizing routes and managing the fleet.
* **vLLM** is the **Jet Engine** on every plane, ensuring maximum speed and efficiency for the individual unit.
====