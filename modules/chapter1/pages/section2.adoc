// PAGE 2: THE PROBLEM - WHY YOUR CLUSTER IS CRYING FOR HELP
//======================================================================

= Round Robin versus LLM-Way

[WARNING.siren]
.A TALE OF TWO LOAD BALANCERS
====
Your Kubernetes cluster is powerful, but it's not a mind reader. When it comes to LLMs, the old ways just don't work. Let's see why...
====

=== The Clueless Load Balancer (The Old Way)

Imagine a traffic cop sending sports cars and semi-trucks down the same narrow lane, one after the other. Chaos! That's what a normal **round-robin** load balancer does to your LLM.

[literal]
....
  +-------------------------------+
  | Round-Robin LB                |
  +-------------------------------+
       |           |         |
       V           V         V
    [GPU 1]     [GPU 2]   [GPU 3]
  (95% busy!) (10% busy) (Crying)
....

* **Result:** One GPU gets slammed with a huge request (a semi-truck), while others sit idle. Latency skyrockets, and your money burns.

=== The Genius Scheduler (The LLM-D Way)

Now, imagine a logistics expert who knows exactly what's in every truck and which loading dock is free. That's the **LLM-D Inference Scheduler**.

[literal]
....
  +-------------------------+
  | LLM-D Scheduler         |
  +-------------------------+
      |         |         |
      V         V         V
    [GPU 1]  [GPU 2]   [GPU 3]
  (Balanced) (Happy) (Efficient)
....

* **Result:** Requests are routed intelligently based on their size and the real-time state of the cluster. Every GPU does its fair share. Performance soars.

[CAUTION.brain]
====
**The Big Secret: LLMs are STATEFUL!** They create a **KV Cache** (think of it as short-term memory) for every conversation. If you route a follow-up question to a GPU that doesn't have that memory... BAM! You have to re-calculate everything. LLM-D is smart enough to remember where the conversation is happening.
====

'''
'''