= Hands-on Lab: llm-d - QuickStart Deployment

This lab guides you through deploying, comparing, and validating a scalable and efficient inference stack on your OpenShift cluster. You will not just deploy llm-d, you will prove its value.

[IMPORTANT.icon-info-circle]
.The "Why Take This Lab?"

The answer is Performance, TCO, and Security. This lab focuses on the first two. You will learn how to:

Prove Guaranteed Performance: You will visually identify and eliminate "hotspots" and queue-blocking to prove you can meet your SLOs.

Prove Lowered TCO: You will find the "Golden Signals" in OpenShift Monitoring that directly translate to cost savings and maximized GPU utilization.

Run Securely: You will set the foundation for enterprise-grade authentication, ensuring your proprietary data never leaves your control.

== Lab Overview

This hands-on lab follows a "compare-and-contrast" model. We will first deploy a vLLM model using a basic Kubernetes scheduler and observe its limitations. Then, we will deploy the exact same model using the llm-d Intelligent Inference Scheduler ("Well-Lit Path 1") and use OpenShift's monitoring tools to see the difference.

This guide assumes you have already completed the "Lab Environment Setup" and have a cluster ready.

=== Your Starting Point: Lab Prerequisites

OpenShift Cluster 4.19.19+ (RHOAI 2.25+).

You have cluster-admin permissions.

You have at least one GPU-enabled node (e.g., g6.xlarge, L40S, A100).

The NVIDIA GPU Operator is installed and running.

The Authorino Operator is installed and running.

LeaderWorkerSet Operator provides the ability to deploy a LWS in OpenShift.

LeaderWorkerSet: An API for deploying a group of pods as a unit of replication. It aims to address common deployment patterns of AI/ML inference workloads, especially multi-host inference workloads where the LLM will be sharded and run across multiple devices on multiple nodes.


'''

== Part 1: Cluster & Client Setup

Before deploying the model, we must prepare the cluster to run llm-d dependencies and configure your client tools.

. Install the OpenShift Web Terminal
+
The Web Terminal in the OpenShift console is the easiest way to run these commands. If you have not already, install the "Web Terminal" operator from OperatorHub.
+
[NOTE.icon-lightbulb]

If the Web Terminal icon (a small *>_* in the top-right console) doesn't appear after installation, reload your browser page.

. To ensure your web terminal has all necessary tools (like helm and yq), apply the enhanced web terminal configuration:
+
[source,bash]

oc apply -k https://github.com/redhat-na-ssa/llm-d-demo/demo/web-terminal


. Clone the Deployment Repository
+
This demo repository contains all the GitOps manifests needed for the lab.
+
[source,bash]

git clone https://github.com/redhat-na-ssa/llm-d-demo.git
cd llm-d-demo

. Scale Your GPU Nodes (If Necessary)
+
This lab requires at least one GPU node. If you used the RHDP lab setup, you may have already created a g6.xlarge machineset. Let's ensure it's scaled to 1.
+
[source,bash]
.Check your machinesets

oc get machineset -n openshift-machine-api

Scale your GPU machineset to 1 (if not already)

(Replace 'YOUR-GPU-MACHINESET' with the name from the command above)

oc scale machineset YOUR-GPU-MACHINESET -n openshift-machine-api --replicas=1

. Install llm-d Cluster Dependencies
+
This kustomize (-k) command will apply all the necessary prerequisites for llm-d on OCP 4.19+, including the Gateway API CRDs and other dependencies.
+
[source,bash,subs="quotes,macros"]

Run this from inside the 'llm-d-demo' git repo

until oc apply -k gitops/ocp-4.19; do : ; done

'''

== Part 2: The "Basic" Deployment (Our Baseline)

First, we'll deploy a model the "basic" way, using a standard Kubernetes Service for load balancing. This will create the "hotspot" problem we want to solve.

. Create the Demo Project
+
[source,bash]

oc new-project demo-llm

. Create a HuggingFace Token Secret
+
(If you haven't already from the lab setup)
+
[source,bash,subs="quotes,macros"]

Replace YOUR_TOKEN_HERE with your actual HF token

oc create secret generic hf-token 

--from-literal=token=YOUR_TOKEN_HERE 

--namespace demo-llm

. Deploy the "Basic" Model
+
This manifest (from the demo repo) deploys a vLLM model as a standard Deployment and exposes it with a Service. It does not use the llm-d scheduler.
+
[source,bash,subs="quotes,macros"]

Run this from inside the 'llm-d-demo' git repo

until oc apply -k demo/llm-basic; do : ; done

. Wait for the "Basic" Pods to be Ready
+
[source,bash]

Wait for the STATUS to show 'Running'

oc get pods -n demo-llm -l app=llm-basic -w

You should see 2 pods running, e.g., llm-basic-deployment-....

'''

== Part 3: The "Smart" Deployment (Path 1: Intelligent Scheduler)

Now, we will deploy the exact same model, but this time using the llm-d QuickStart. This will deploy it using the ModelService CRD, which automatically configures the intelligent KServe "Next GenAI" Controller and the Inference scheduler (EPP).

. Deploy the llm-d QuickStart
+
This command deploys the LLMInferenceService for the gpt-oss-20b model.
+
[source,bash,subs="quotes,macros"]

Run this from inside the 'llm-d-demo' git repo

until oc apply -k demo/llm-d; do : ; done

. Wait for the llm-d Pods to be Ready
+
[source,bash]

Wait for the STATUS to show 'Ready' or 'Available'

oc get llminferenceservice -n demo-llm -w

This will create a new set of pods for the llm-d deployment. You now have two separate deployments of the same model.

'''

== Part 4: Prove the Value (Observability)

This is the most important step. We will run a load test and use the OpenShift Monitoring dashboard to visually prove the difference between the "basic" and "smart" deployments.

. Get the Inference URLs
+
The "smart" service is exposed via the main OpenShift AI Ingress Gateway. We'll expose the "basic" one with a simple Route.
+
[source,bash,subs="quotes,macros"]

1. Get the "SMART" URL (llm-d)

export SMART_URL=$( \
oc -n openshift-ingress get gateway openshift-ai-inference \
-o jsonpath='{.status.addresses[0].value}' \
)
export SMART_ENDPOINT="http://${SMART_URL}/demo-llm/gpt-oss-20b/v1/completions"

2. Expose the "BASIC" service and get its URL

oc expose service llm-basic-service -n demo-llm
export BASIC_URL=$(oc get route llm-basic-service -n demo-llm -o jsonpath='{.spec.host}')
export BASIC_ENDPOINT="http://${BASIC_URL}/v1/completions"

echo "SMART (llm-d) URL: $SMART_ENDPOINT"
echo "BASIC (K8s) URL: $BASIC_ENDPOINT"

. Navigate to the GPU Dashboard
+
In the OpenShift Console, navigate to Observe > Dashboards.
+
In the "Dashboards" search bar, type "NVIDIA" and select the "NVIDIA DCGM Exporter" dashboard. This dashboard shows you the real-time metrics from your GPUs.

. Run a Load Test (and Watch the Dashboard)
+
We will use a simple for loop to send 20 requests to our "basic" service. While this runs, keep your eye on the "DCGM" dashboard.
+
[source,bash,subs="attributes+"]

Set the prompt

export LLM="openai/gpt-oss-20b"
export PROMPT="Explain the difference between supervised and unsupervised learning in machine learning."

Run 20 requests against the "BASIC" endpoint

for i in {1..20}; do 

curl -s -X POST $BASIC_ENDPOINT \
-H "Content-Type: application/json" \
-d '{ "model": "'${LLM}'", "prompt": "'${PROMPT}'", "max_tokens": 50 }' 

-o /dev/null & 

done

What to Observe (The "Hotspot"): On the dashboard, look at the "GPU Utilization" panel. You will see one GPU pod spike to 100% utilization, while the other pod sits idle. This is the "basic" scheduler's round-robin failing. It sends all requests to one pod until it's full, creating a hotspot.

. Run the Same Load Test on the "Smart" Scheduler
+
Now, run the exact same test against the llm-d endpoint.
+
[source,bash,subs="attributes+"]

Run 20 requests against the "SMART" (llm-d) endpoint

NOTE: This endpoint requires authentication. We'll create a token.

export SA_NAME=llm-user
export TEST_NS=demo-llm
oc create sa ${SA_NAME} -n ${TEST_NS}

(We'll create the full RBAC in the next step, for now just get the token)

export TEST_TOKEN=$(oc create token ${SA_NAME} -n ${TEST_NS})

for i in {1..20}; do 

curl -s -X POST $SMART_ENDPOINT \
-H "Content-Type: application/json" \
-H "Authorization: Bearer ${TEST_TOKEN}" \
-d '{ "model": "'${LLM}'", "prompt": "'${PROMPT}'", "max_tokens": 50 }' 

-o /dev/null & 

done

What to Observe (The Value): Look at the "GPU Utilization" panel again. You will see the llm-d scheduler distribute the load evenly across both pods. Both pods will show ~50% utilization instead of one at 100% and one at 0%.
+
You have just proven the TCO value. You are eliminating idle, expensive hardware and maximizing your utilization.

. Find the "Golden TCO Signal"
+
In the Observe > Metrics tab, run this query:
+
vllm_llmd_kv_cache_hit_rate
+
You will see a metric for your llm-d service. This is your #1 TCO METRIC. A high hit rate means llm-d is reusing the KV Cache, skipping the expensive "Prefill" step entirely. This is a direct measure of your cost savings and ROI.

'''

== Part 5: (Optional) Secure the Endpoint

In a real environment, you need secure, authenticated access. llm-d on RHOAI is secure by default. This step shows you how to grant a ServiceAccount the minimum required permissions to call the model.

. Create the RBAC (Role and RoleBinding)
+
This is the key step. We create a Role that can only get the llminferenceservices resource, and a RoleBinding to give that permission to our llm-user ServiceAccount.
+
[source,text]

cat <<EOF | oc apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
name: llm-inferenceservice-reader
namespace: $TEST_NS
rules:

apiGroups: ["serving.kserve.io"]
resources: ["llminferenceservices"]
verbs: ["get"]

resourceNames: ["gpt-oss-20b"] # Optional: Uncomment to restrict to one service

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
name: llm-inferenceservice-reader-binding
namespace: $TEST_NS
subjects:

kind: ServiceAccount
name: $SA_NAME
namespace: $TEST_NS
roleRef:
kind: Role
name: llm-inferenceservice-reader
apiGroup: rbac.authorization.k8s.io
EOF

. Send a Fully Authenticated Request
+
Now, we send the same request as before. It will work, and you'll get a real response.
+
[source,bash,subs="attributes+"]

curl -s -X POST $SMART_ENDPOINT \
-H "Content-Type: application/json" \
-H "Authorization: Bearer ${TEST_TOKEN}" \
-d '{ "model": "'${LLM}'", "prompt": "'${PROMPT}'", "max_tokens": 200, "temperature": 0.7 }' | jq .choices[0].text

[TIP.icon-check-circle]
.Success!

You should now see a 200 OK response and the JSON output from the model. You have successfully deployed, validated, and secured an enterprise-grade AI service that is proven to be more cost-effective than a basic deployment.

'''

== Part 6: Cleanup

To remove the resources from this lab:

[source,bash,subs="quotes,macros"]

Delete the "smart" (llm-d) deployment

oc delete -k demo/llm-d

Delete the "basic" deployment

oc delete -k demo/llm-basic

Delete the ServiceAccount and RBAC

oc delete sa ${SA_NAME} -n ${TEST_NS}
oc delete role llm-inferenceservice-reader -n ${TEST_NS}
oc delete rolebinding llm-inferenceservice-reader-binding -n ${TEST_NS}