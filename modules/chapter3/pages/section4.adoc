= OpenShift AI: Deep Dive on KV Cache Management

[TIP.tada]
.The KV Cache "Litmus Test"

The KV Cache is the #1 bottleneck for LLM performance. If you nod your head to these points, this deep dive is for you.

 - [x] ðŸ“ˆ You run high-concurrency services with many simultaneous users.

 - [x] ðŸ—£ï¸ Your application is a chatbot or RAG system that requires long conversation histories (context).

 - [x] ðŸ¢ You are suffering from high Time-To-First-Token (TTFT) as your request queues get backed up.

 - [x] ðŸ’¸ Your expensive GPU VRAM is constantly full, limiting the total number of users you can serve.

 - [x] ðŸš€ You want to achieve the absolute lowest latency and highest throughput possible from your hardware.

'''
'''

Architectural Advantage:
OpenShift AI's Distributed Inference (powered by LLM-D) treats the KV Cache as a first-class, orchestrated component, not just a blob in memory. Its primary strength is a three-layered approach:
. An intelligent routing layer (the Inference Gateway) that knows which pods have which caches.
. A disaggregated serving layer (vLLM) that can physically separate the compute-heavy Prefill pods from the cache-heavy Decode pods.
. A pluggable cache hierarchy that allows the cache to be offloaded from expensive GPU VRAM to CPU RAM or even a distributed cache pool like Redis.

Performance Drivers:
This architecture's "magic" comes from two key techniques:
. Prefix Cache-Aware Routing: The gateway intelligently inspects requests. If it sees a request for a conversation it has already started, it routes that request directly to the pod that already holds its KV Cache. This completely skips the expensive "Prefill" step, massively reducing latency (TTFT) and GPU load.
. Disaggregated Caching: By separating Prefill and Decode pods, you can use a shared cache (like Redis). A Prefill pod can write to the cache, and a different Decode pod can read from it. This prevents a single long-running job from blocking all other users.

The Strategic Goal:
The goal is to stop re-computing the same context over and over. Every time you skip a "Prefill," you reduce latency for the user and free up a GPU cycle for someone else. This directly lowers your TCO (you serve more users on the same hardware) and guarantees your performance SLOs.

'''
'''
[#deep-dive]
== Deep Dive: Understanding the KV Cache

Now that you understand why the cache is so important, let's explore what it is and the strategies to manage it.

=== What is the KV Cache?

The KV Cache is the model's "short-term memory."

When you send a prompt (like "What is the capital of France?"), the model performs two phases:

. Prefill Phase: It processes your prompt all at once. As it does, it generates intermediate data (the Keys and Values, or "KV") that represent the meaning and context of your prompt. This data is the KV Cache. This phase is compute-heavy and slow.
. Decode Phase: To generate an answer ("The", "capital", "is", "Paris"), the model looks at the KV Cache and generates one new token at a time. This phase is memory-bandwidth-heavy but very fast.

The KV Cache lives in the most expensive, limited resource you have: GPU VRAM.

=== Why is the Cache a Performance Bottleneck?

The KV Cache is the root of two major performance problems:

[cols="1a,1a"]
|===
|Problem 1: VRAM Exhaustion |Problem 2: The "Prefill" Bottleneck

|The KV Cache is huge. A single user with a long conversation history (like a RAG chatbot) can consume gigabytes of VRAM. This limits how many users can be served at the same time on one GPU.
|The "Prefill" step (which creates the cache) is slow and compute-intensive. In a traditional system, a new, long Prefill request will get in line and block all the fast Decode requests waiting behind it. This causes a "head-of-line blocking" problem and makes your whole service feel laggy.
|===

OpenShift AI's distributed inference provides two main strategies to defeat both of these problems.

=== Caching Strategy 1: Independent Caching (VRAM to RAM Offload)

This is a "North-South" caching strategy, meaning the cache data moves within a single pod.

What it is: The vLLM engine inside the pod intelligently offloads "cold" (infrequently used) KV Caches from expensive GPU VRAM to more abundant, cheaper CPU RAM.

How it works: When a new user request comes in, its "cold" cache is quickly swapped from CPU RAM back into VRAM for the fast Decode phase.

When to use it: This is the default and is perfect for most workloads. It's an automatic way to fit more users onto a single GPU, maximizing your TCO.

[NOTE.info]
.Analogy: A Chef's Fridge

Think of GPU VRAM as your small, tableside prep fridge (fast access) and CPU RAM as your main walk-in cooler (large capacity).

This strategy keeps only the active ingredients (hot caches) in the small fridge and stores everything else in the walk-in. This is far more efficient than trying to fit your entire inventory tableside.

=== Caching Strategy 2: Shared Caching (Distributed Cache Pool)

This is an "East-West" caching strategy, meaning the cache data moves between pods.

What it is: This strategy is used when Prefill and Decode are fully disaggregated (running on different pods). A shared, external cache pool (like Redis) is used as the "single source of truth" for the KV Caches.

How it works:
. A user sends a new prompt.
. The Gateway routes it to a Prefill pod.
. The Prefill pod does the heavy compute and writes the resulting KV Cache to the Shared Redis Cache.
. The Gateway then routes all future Decode requests for that user to any available Decode pod, which reads the cache from Redis.

When to use it: This is an advanced strategy for extreme-scale, high-concurrency workloads. It's more complex (requires a Redis cluster and high-speed networking) but offers the ultimate in flexibility, as you can scale your Prefill compute and Decode memory resources completely independently.

== Summary: When to Use Each Strategy

[cols="1a,2a,2a"]
|===
|Strategy |What It Solves |When to Use It (Implementation)

|Independent Caching
(VRAM -> RAM Offload)
|Problem: VRAM Exhaustion
|Default / Recommended: This is the standard, high-performance mode. It maximizes the user capacity of your individual GPU nodes.

|Shared Caching
(Distributed Pool)
|Problem: "Head-of-Line" Blocking
|Advanced / High-Scale: Use this when you are running a massive, multi-tenant service and need to independently scale your Prefill compute (for long prompts) from your Decode capacity (for high user count).
|===