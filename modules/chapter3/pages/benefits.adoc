= llm-d Use Cases and Core Benefits


== The llm-d Litmus Test: Business Problems Solved

llm-d is not merely a technical tool; it is a solution engineered to address specific, expensive business problems encountered when scaling Generative AI projects on kubernetes from experimentation to production. If your organization identifies with these symptoms, `llm-d` is designed to provide the necessary economic and operational viability.

.Common Business Problems Solved by llm-d
|===
| Symptom (What You’re Seeing) | Problem/Mandate | How llm-d Solves It

| **Skyrocketing AI OpEx & Poor ROI**
| "Our GPU budget is out of control. We keep buying more hardware, but our user capacity doesn't double. We can’t make this project economically viable."
| **Intelligent Scheduling (Path 1) and KV Cache Management (Path 4)** stop waste, allowing you to pack more users and models onto the same hardware, directly lowering Total Cost of Ownership (TCO) and improving cost-per-query.

| **Inconsistent or Slow AI Applications**
| "Users complain the chatbot is 'slow to start' (High TTFT) or 'stutters' during a long answer (High TPOT). Our RAG app times out under load."
| **P/D Disaggregation (Path 2) and Smart Scheduling** eliminate hotspots and optimize the pipeline for fast, consistent responses, enabling you to meet Service Level Objectives (SLOs).

| **Data Sovereignty & Security Mandates**
| "Legal and compliance teams have blocked the use of third-party APIs. We must build this in-house, but we can't sacrifice performance."
| `llm-d` runs entirely inside your **secure OpenShift cluster**. Your proprietary data, RAG documents, and sensitive prompts never leave your private or hybrid cloud environment, delivering public-cloud performance with private-cloud security.

| **Inability to Run Next-Gen Models**
| "We are stuck on smaller models. We want to use a new 8x7B (MoE) model to stay competitive, but we have no way to serve it efficiently."
| **Wide Expert Parallelism (Path 3)** is specifically designed to handle the brain-melting complexity of serving massive Mixture-of-Experts (MoE) models across many nodes, giving you a clear path to utilize next-generation AI.
|===

== llm-d Architectural Advantage and Performance Drivers

`llm-d`’s significant performance improvements are the direct result of implementing advanced distributed systems techniques within a clean, cloud-native, three-layered architecture.


image::llm-d-benefits.png[llm-d Advantage & Performance Drivers, 700, align="center"]

== The Four "Well-Lit Paths" of Distributed Inference

Distributed inference with `llm-d` truly shines by offering a set of powerful, production-ready techniques known as the "Well-Lit Paths" to supercharge your inference workloads.

1.  **Path #1: Intelligent Inference Scheduling (Flagship Feature)**
    *   This is the essential "on-ramp" to distributed inference, adding intelligent routing to any model served with OpenShift AI using vLLM.
    *   **Why Basic Schedulers Fail:** Traditional schedulers (like round-robin) fail because LLM inference is stateful (using KV caches), requests have variable costs (long RAG prompt vs. short chat query), and basic distribution creates "hotspots" while wasting expensive, idle hardware.
    *   **How llm-d Solves It:** `llm-d` replaces the load balancer with an intelligent, metrics-aware **KServe Gateway**. This gateway actively monitors real-time load, queue depth, and in-flight requests. It uses a **two-step "Filter-and-Score" process** to find the perfect home for every request, prioritizing availability, shortest queue, and KV cache affinity.

2.  **Path #2: Prefill/Decode (P/D) Disaggregation (Microservices for AI)**
    *   This path splits the inference process, preventing one server from juggling both compute-heavy Prefill and memory-heavy Decode.
    *   It separates the workload into specialized teams: the **Prefill Team (A+ at compute)** and the **Decode Team (Masters of Memory)**, resulting in a massive **25-50% performance boost** for large models. This separation stops long prompts from blocking the queue, maintaining responsiveness.

3.  **Path #3: Wide Expert Parallelism**
    *   This path is reserved for the **"Boss Level"**—serving gargantuan Mixture-of-Experts (MoE) models.
    *   `llm-d` handles the complexity of splitting these giant models across many nodes and manages the high-speed networking required to make them work as one cohesive unit.

4.  **Path #4: KV Cache Management**
    *   The "short-term memory" (KV Cache) lives in super-expensive GPU RAM.
    *   The `llm-d-kv-cache-manager` is designed to offload this data to **cheaper, more abundant storage**.
    *   This system supports **North-South Caching** (moving data to CPU RAM, like keeping a book on your personal desk) and **East-West Caching** (putting the book on a shared table/Redis/Storage for instant team access).

### Who is This For? Target Audience and Positioning

`llm-d` is a specialist solution designed for a specific set of users, deeply integrated with Kubernetes and requiring a high level of infrastructure expertise.

|===
| Persona | Role and Focus | Key Skills Required

| **Persona 1: The Operator**
| The primary user (Platform/SRE Team) who installs, configures, and manages `llm-d`.
| *Strong Kubernetes/OpenShift Expertise* (Operators, networking, security contexts). *Helm & GitOps Fluency*. *Observability Skills* (Prometheus/Grafana for monitoring GPU and scheduler metrics). *Hardware Awareness* (GPU hardware, NVIDIA drivers).

| **Persona 2: The User**
| The consumer (Data Scientists & App Developers) who utilizes the deployed service.
| Needs to be **API-Driven**, only requiring knowledge of the **OpenAI-compatible API endpoint** provided by the platform team. They are abstracted away from the deep infrastructure complexity.
|===

[IMPORTANT]
**Competitive Positioning:** `llm-d` occupies a strategic niche by prioritizing **infrastructure efficiency and TCO for LLM-specific workloads**. It is best understood as a specialist solution for platform and SRE teams managing large, multi-tenant GPU clusters on Kubernetes, differentiating it from general-purpose, multi-framework servers (like NVIDIA Triton) or application-centric, developer-focused frameworks (like Ray Serve).
```