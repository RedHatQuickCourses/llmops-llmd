= xyz
Foundations of OpenShift AI Distributed Inference:toc: left:icons: fontWelcome! Your journey to building a scalable, cost-effective, and enterprise-grade "AI Factory" starts here.This guide is the conceptual "Front Door" for the OpenShift AI Distributed Inference lab. Before you dive into the hands-on deployment in README.md, it's critical to understand the challenge this technology solves and the value it provides.[NOTE]
.What You'll Learn in This GuideThe Core Challenge: Why traditional scaling methods fail for AI.The "Why Buy?": The business value (TCO, Performance, Security) of this solution.The Core Solution: How OpenShift AI transforms AI workloads.The "Well-Lit Paths": The key strategies you'll use in the lab.====== The Challenge: "Monolithic" AI WorkloadsToday, many AI inference workloads are deployed as "monolithic" black boxes. This approach is simple to start, but it's resource-blind and AI-unaware.This "black box" method creates several critical problems:It's impossible to see what is causing a bottleneck.It's inefficient, as you must scale the entire container for all types of traffic.It's inflexible, leading to high costs and missed performance goals (SLOs).== Why Traditional Scaling Fails for AIA standard Kubernetes load balancer (like round-robin) is fundamentally ill-suited for AI. This failure is due to two core problems:=== 1. Varied Request "Shapes"LLM requests are not uniform. A simple chat reply is completely different from a 10-page document summary..Visual: A "Dumb" Load Balancer
[sidebar]Input Traffic:Request A (Small): "Translate 'hello'"Request B (Large): "Summarize this 10-page doc"Action:The load balancer sends Request A to Server 1 and Request B to Server 2.Result:
Server 1 is done in 1 second and is now IDLE.
Server 2 is 100% busy for 30 seconds, BLOCKING all new requests.This "shape-blind" routing leads to massive inefficiencies, resource bottlenecks, and idle, wasted GPU cycles.=== 2. "Stateless" Routing for a "Stateful" WorkloadLLM inference is a stateful operation. To generate a reply, the model must have the conversation history (the KV Cache) in its memory. Re-calculating this cache is the most expensive part of the process..Visual: A "Stateless" Router
[sidebar]Input Traffic:Msg 1 (Chat A): "What's the capital of France?"Msg 2 (Chat A): "What is its population?"Action:Msg 1 hits Server 1 (which now holds the KV Cache for Chat A).The "stateless" router sends Msg 2 to Server 2.Result:
Server 2 has no context! It must re-calculate the entire cache for Chat A from scratch. This is a massive waste of GPU cycles and dramatically slows down the response (Time-To-First-Token).== The Solution: How OpenShift AI DeliversWhen you can get cheap tokens from a public API, why build your own AI factory?The answer is Performance, Security, and Cost (TCO) across any infrastructure or cloud provider.OpenShift AI's distributed inference feature, powered by LLM-D, transforms the monolithic "black box" into an intelligent, distributed system.=== 1. Guarantees Performance & SLOsBy intelligently separating large "Prefill" jobs (like Request B) from small "Decode" jobs (like Request A), nothing gets blocked. Its "Prefix Cache-Aware" routing (the opposite of stateless) sends follow-up messages to the correct server with the cache, nearly eliminating redundant calculations.=== 2. Lowers Your Total Cost of Ownership (TCO)This intelligent routing stops the waste. You stop paying for idle GPUs. By maximizing the utilization of your hardware, you get the full value from your investment, which is the key to cost-effective token generation on-premise.=== 3. Runs Securely on Your Hybrid CloudThis entire stack runs on your OpenShift cluster, on your infrastructure, whether on-prem or in the cloud. Your proprietary data never leaves your control, satisfying the strictest security and compliance needs.== Your Next Step: The "Well-Lit Paths"This lab will guide you through the most important of these "well-lit paths": Inference Scheduling.This is the core strategy that provides most of the value:Disaggregated Inference: The core concept of separating Prefill (big jobs) from Decode (small jobs).Smarter Scheduling: The "brain" (Inference Gateway) that routes traffic to the correct pod based on load and KV Cache state.Scaling MoE: An advanced path for running massive Mixture of Experts (MoE) models (not covered in this initial lab).You now understand the "Why" behind OpenShift AI's distributed inference. It's time to build it.Please proceed to the README.md file in this repository to begin the hands-on lab.